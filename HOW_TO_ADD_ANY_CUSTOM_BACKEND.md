# How to Add Any Custom Backend Through TUI

**Comprehensive Implementation Guide for AI Agents**  
*Version: 1.0*  
*Last Updated: January 27, 2026*  
*Target: AI Implementation Agents*

---

## ðŸ“‹ Table of Contents

1. [Overview](#overview)
2. [Architecture Design](#architecture-design)
3. [Phase 1: Backend Addition Wizard](#phase-1-backend-addition-wizard)
4. [Phase 2: Backend Configuration Interface](#phase-2-backend-configuration-interface)
5. [Phase 3: Code Generation System](#phase-3-code-generation-system)
6. [Phase 4: Testing & Validation Interface](#phase-4-testing--validation-interface)
7. [Phase 5: Integration & Deployment](#phase-5-integration--deployment)
8. [Complete File Structure](#complete-file-structure)
9. [Implementation Checklist](#implementation-checklist)
10. [Testing Procedures](#testing-procedures)

---

## Overview

### Purpose

This document provides a complete, AI-implementable specification for adding an **"AI-Powered Custom Backend Addition System"** to the Proxima TUI. This system combines a traditional step-by-step wizard with **AI-assisted conversational interface** to allow users to add new quantum simulator backends through natural language descriptions and intelligent code generation.

### Design Philosophy

**Inspiration Sources:**
- **Crush Agent**: 
  - Multi-model LLM support (OpenAI, Anthropic, local models)
  - Conversational interface with context awareness
  - JSON-based configuration with environment variable support
  - Beautiful terminal UI with command palette
  - Model switching and provider management
  
- **OpenCode Agent**: 
  - Interactive wizards with AI assistance
  - Code generation and validation
  - Built-in agent for complex tasks
  - Terminal-first design philosophy
  - LSP integration for intelligent suggestions

**Key Principles:**
1. **Zero-Code Experience**: Users should not need to write Python code manually
2. **AI-Assisted Creation**: Use LLM to generate backend code from natural language descriptions
3. **Dual Mode Interface**: Traditional wizard OR conversational AI mode
4. **Guided Process**: Step-by-step wizard with clear instructions
5. **Validation at Every Step**: Real-time feedback and error prevention
6. **Smart Defaults**: AI-powered suggestions and intelligent pre-filling
7. **Reversible Actions**: Ability to go back and modify choices
8. **Beautiful UI**: Clean, professional terminal interface with proper navigation
9. **Model Flexibility**: Support for local LLMs (Ollama, LM Studio) and API-based models (OpenAI, Anthropic)
10. **Context-Aware**: Remember previous backends and suggest improvements

### User Experience Flow

**Mode 1: Traditional Wizard Flow**
```
Dashboard â†’ Backends Screen â†’ [Add Backend] Button â†’ [Choose Mode]
    â†“
[Wizard Mode Selected]
    â†“
Backend Addition Wizard (7 Steps)
    â†“
Step 1: Welcome & Backend Type Selection
Step 2: Basic Information Input
Step 3: Capabilities Configuration
Step 4: Gate Mapping Configuration
Step 5: Code Template Selection & Customization
Step 6: Testing & Validation
Step 7: Review & Deployment
    â†“
Backend Added Successfully â†’ Backend Management Screen
```

**Mode 2: AI-Assisted Conversational Flow**
```
Dashboard â†’ Backends Screen â†’ [Add Backend] Button â†’ [Choose Mode]
    â†“
[AI Assistant Mode Selected]
    â†“
LLM Configuration & Model Selection
    â†“
Conversational Interface:
  User: "I want to add a Python-based quantum simulator..."
  AI: Analyzes description â†’ Asks clarifying questions â†’ Generates configuration
    â†“
AI generates backend code automatically
    â†“
Review & Refinement (User can chat to modify)
    â†“
Testing & Validation (AI explains results)
    â†“
Deployment
    â†“
Backend Added Successfully â†’ Backend Management Screen
```

**Mode 3: Hybrid Flow**
```
Use AI assistance at any step of the wizard
  - Ask AI to fill current step
  - Get AI suggestions for capabilities
  - Generate code snippets with AI
  - Debug errors with AI help
```

---

## Architecture Design

### LLM Integration Architecture

```
src/proxima/
â”œâ”€â”€ llm/                                # NEW - LLM integration layer
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ providers.py                   # LLM provider management
â”‚   â”œâ”€â”€ models.py                      # Model configuration
â”‚   â”œâ”€â”€ backend_agent.py               # Specialized agent for backend creation
â”‚   â”œâ”€â”€ prompts/                       # Prompt templates
â”‚   â”‚   â”œâ”€â”€ backend_generation.txt
â”‚   â”‚   â”œâ”€â”€ code_refinement.txt
â”‚   â”‚   â”œâ”€â”€ capability_suggestion.txt
â”‚   â”‚   â””â”€â”€ error_debugging.txt
â”‚   â””â”€â”€ config/
â”‚       â”œâ”€â”€ providers.json             # LLM provider configs
â”‚       â””â”€â”€ models.json                # Available models
â”‚
â”œâ”€â”€ tui/
â”‚   â”œâ”€â”€ screens/
â”‚   â”‚   â”œâ”€â”€ backends.py                # Enhanced with "Add Backend" & mode selection
â”‚   â”‚   â”œâ”€â”€ backend_wizard.py          # Traditional wizard
â”‚   â”‚   â”œâ”€â”€ backend_ai_chat.py         # NEW - AI conversational interface
â”‚   â”‚   â””â”€â”€ llm_settings.py            # NEW - LLM configuration screen
â”‚   â”‚
â”‚   â”œâ”€â”€ dialogs/
â”‚   â”‚   â”œâ”€â”€ backend_wizard/            # Traditional wizard dialogs
â”‚   â”‚   â””â”€â”€ backend_ai_assistant/      # NEW - AI assistant dialogs
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ chat_interface.py      # Chat UI component
â”‚   â”‚       â”œâ”€â”€ model_selector.py      # Model selection dialog
â”‚   â”‚       â”œâ”€â”€ provider_config.py     # Provider configuration
â”‚   â”‚       â””â”€â”€ ai_suggestions.py      # AI suggestion display
â”‚   â”‚
â”‚   â”œâ”€â”€ widgets/
â”‚   â”‚   â”œâ”€â”€ chat_widget.py             # NEW - Chat message display
â”‚   â”‚   â”œâ”€â”€ model_status.py            # NEW - Model status indicator
â”‚   â”‚   â””â”€â”€ ai_thinking.py             # NEW - AI processing indicator
â”‚
â””â”€â”€ config/
    â””â”€â”€ llm_config.yaml                # NEW - LLM configuration
```

### Component Structure

```
src/proxima/tui/
â”œâ”€â”€ screens/
â”‚   â”œâ”€â”€ backends.py                    # Enhanced with "Add Backend" button & mode selection
â”‚   â”œâ”€â”€ backend_wizard.py              # Traditional wizard screen
â”‚   â””â”€â”€ backend_ai_chat.py             # NEW - AI conversational interface
â”œâ”€â”€ dialogs/
â”‚   â”œâ”€â”€ backend_wizard/                # NEW - Wizard dialog modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ step_welcome.py            # Step 1: Welcome screen
â”‚   â”‚   â”œâ”€â”€ step_basic_info.py         # Step 2: Basic info form
â”‚   â”‚   â”œâ”€â”€ step_capabilities.py       # Step 3: Capabilities selection
â”‚   â”‚   â”œâ”€â”€ step_gate_mapping.py       # Step 4: Gate configuration
â”‚   â”‚   â”œâ”€â”€ step_code_template.py      # Step 5: Template selection
â”‚   â”‚   â”œâ”€â”€ step_testing.py            # Step 6: Testing interface
â”‚   â”‚   â”œâ”€â”€ step_review.py             # Step 7: Review & deploy
â”‚   â”‚   â””â”€â”€ wizard_state.py            # Shared state management
â”‚   â””â”€â”€ backend_dialogs.py             # Enhanced existing dialogs
â”œâ”€â”€ controllers/
â”‚   â””â”€â”€ backend_generator.py           # NEW - Code generation logic
â”œâ”€â”€ widgets/
â”‚   â””â”€â”€ wizard_navigation.py           # NEW - Navigation component
â””â”€â”€ utils/
    â””â”€â”€ backend_templates.py           # NEW - Template library

src/proxima/backends/
â””â”€â”€ _generated/                        # NEW - Auto-generated backends
    â””â”€â”€ .gitignore                     # Ignore generated files
```

### Data Flow

```
User Input (TUI Forms)
    â†“
Wizard State Manager (wizard_state.py)
    â†“
Validation Engine (Per-step validators)
    â†“
Code Generator (backend_generator.py)
    â†“
Template Engine (backend_templates.py)
    â†“
File System Writer (Creates adapter, normalizer, __init__.py)
    â†“
Registry Integration (Auto-registers backend)
    â†“
Testing Interface (Validates functionality)
    â†“
Success Notification
```

### LLM Provider Configuration

**Configuration File: `config/llm_config.yaml`**

```yaml
# LLM Configuration for Backend Creation
llm:
  # Default provider and model
  default_provider: "openai"  # or "anthropic", "ollama", "local"
  default_model: "gpt-4"
  
  # Enable/disable AI features
  enabled: true
  ai_wizard_mode: true
  ai_suggestions: true
  
  # Provider configurations
  providers:
    openai:
      type: "openai"
      api_key_env: "OPENAI_API_KEY"  # Read from environment variable
      base_url: "https://api.openai.com/v1"
      models:
        - id: "gpt-4"
          name: "GPT-4"
          context_window: 128000
          cost_per_1m_tokens: 30
        - id: "gpt-3.5-turbo"
          name: "GPT-3.5 Turbo"
          context_window: 16000
          cost_per_1m_tokens: 1.5
    
    anthropic:
      type: "anthropic"
      api_key_env: "ANTHROPIC_API_KEY"
      base_url: "https://api.anthropic.com/v1"
      models:
        - id: "claude-3-opus-20240229"
          name: "Claude 3 Opus"
          context_window: 200000
          cost_per_1m_tokens: 15
        - id: "claude-3-sonnet-20240229"
          name: "Claude 3 Sonnet"
          context_window: 200000
          cost_per_1m_tokens: 3
    
    ollama:
      type: "ollama"
      base_url: "http://localhost:11434"
      models:
        - id: "codellama:13b"
          name: "CodeLlama 13B"
          context_window: 16000
          local: true
        - id: "deepseek-coder:6.7b"
          name: "DeepSeek Coder 6.7B"
          context_window: 16000
          local: true
    
    lmstudio:
      type: "openai-compatible"
      base_url: "http://localhost:1234/v1"
      models:
        - id: "local-model"
          name: "LM Studio Model"
          context_window: 32000
          local: true
  
  # Prompt configurations
  prompts:
    temperature: 0.7
    max_tokens: 4000
    system_prompt: |
      You are an expert quantum computing backend developer assistant.
      Help users create custom quantum simulator backends for Proxima.
      Generate clean, working Python code following Proxima's architecture.
      Be concise but thorough in explanations.
```

**Environment Variables:**
```bash
# API Keys (store in .env or environment)
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export GROQ_API_KEY="gsk_..."

# Local LLM endpoints
export OLLAMA_HOST="http://localhost:11434"
export LMSTUDIO_HOST="http://localhost:1234"
```

### State Management

**Wizard State Schema:**
```python
@dataclass
class BackendWizardState:
    # Step 1: Backend Type
    backend_type: str = ""  # 'python_library', 'command_line', 'api_server', 'custom'
    
    # Step 2: Basic Information
    backend_name: str = ""  # e.g., "mybackend"
    display_name: str = ""  # e.g., "My Custom Backend"
    version: str = "1.0.0"
    description: str = ""
    author: str = ""
    library_name: str = ""  # Python import name (e.g., "mybackend_lib")
    
    # Step 3: Capabilities
    simulator_types: List[str] = field(default_factory=list)  # ['state_vector', 'density_matrix']
    max_qubits: int = 20
    supports_noise: bool = False
    supports_gpu: bool = False
    supports_batching: bool = False
    custom_features: Dict[str, Any] = field(default_factory=dict)
    
    # Step 4: Gate Mapping
    gate_mapping_mode: str = "auto"  # 'auto', 'manual', 'template'
    supported_gates: List[str] = field(default_factory=list)
    custom_gate_mappings: Dict[str, str] = field(default_factory=dict)
    
    # Step 5: Code Template
    template_type: str = "basic"  # 'basic', 'advanced', 'custom'
    custom_initialization_code: str = ""
    custom_execution_code: str = ""
    
    # Step 6: Testing
    test_circuit: str = "bell_state"
    test_results: Optional[Dict] = None
    validation_passed: bool = False
    
    # Step 7: Review
    files_to_create: List[str] = field(default_factory=list)
    generation_successful: bool = False
    
    # Navigation
    current_step: int = 1
    total_steps: int = 7
    can_proceed: bool = False
    errors: List[str] = field(default_factory=list)
```

---

## Phase 0: Mode Selection & LLM Configuration

### Mode Selection Screen

**File:** `src/proxima/tui/screens/backend_mode_selector.py`

**Purpose:** Let users choose between traditional wizard or AI-assisted mode

**UI Layout:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘               Add Custom Backend - Choose Mode                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘  How would you like to create your backend?                      â•‘
â•‘                                                                  â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘                                                                  â•‘
â•‘  â—‹ ðŸ§™ Traditional Wizard                                         â•‘
â•‘    Step-by-step guided form interface                           â•‘
â•‘    Best for: Users who prefer structured input                  â•‘
â•‘    Time: ~5-10 minutes                                           â•‘
â•‘                                                                  â•‘
â•‘  â—‹ ðŸ¤– AI Assistant (Conversational)                              â•‘
â•‘    Chat with AI to describe your backend                        â•‘
â•‘    Best for: Quick setup with natural language                  â•‘
â•‘    Time: ~2-5 minutes                                            â•‘
â•‘    Requires: LLM API key or local model                         â•‘
â•‘                                                                  â•‘
â•‘  â—‹ ðŸ”€ Hybrid Mode                                                â•‘
â•‘    Use wizard with AI assistance at each step                   â•‘
â•‘    Best for: Guided process with intelligent suggestions        â•‘
â•‘    Time: ~3-7 minutes                                            â•‘
â•‘                                                                  â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘                                                                  â•‘
â•‘  LLM Status:                                                     â•‘
â•‘    Current Provider: OpenAI                                      â•‘
â•‘    Current Model: GPT-4 âœ“ Available                             â•‘
â•‘    [âš™ Configure LLM Settings]                                   â•‘
â•‘                                                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘         [Cancel]              [Continue â†’]                       â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### LLM Configuration Screen

**File:** `src/proxima/tui/screens/llm_settings.py`

**Purpose:** Configure LLM provider and model

**UI Layout:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘               LLM Configuration                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘  Configure AI model for backend generation:                      â•‘
â•‘                                                                  â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘                                                                  â•‘
â•‘  Provider:                                                       â•‘
â•‘  [â–¼ OpenAI                    ]                                 â•‘
â•‘      â€¢ OpenAI (API Key required)                                â•‘
â•‘      â€¢ Anthropic (API Key required)                             â•‘
â•‘      â€¢ Ollama (Local, no API key)                               â•‘
â•‘      â€¢ LM Studio (Local, no API key)                            â•‘
â•‘      â€¢ Groq (API Key required)                                  â•‘
â•‘                                                                  â•‘
â•‘  Model:                                                          â•‘
â•‘  [â–¼ gpt-4                     ]                                 â•‘
â•‘      â€¢ gpt-4 (Best quality, slower)                             â•‘
â•‘      â€¢ gpt-3.5-turbo (Fast, good quality)                       â•‘
â•‘                                                                  â•‘
â•‘  API Key:                                                        â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘  â”‚ sk-...                                         [ðŸ‘ Show]   â”‚ â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘  â„¹ Stored in: ~/.proxima/.env (encrypted)                       â•‘
â•‘                                                                  â•‘
â•‘  [Test Connection]                                               â•‘
â•‘  âœ“ Connection successful! Model is ready.                       â•‘
â•‘                                                                  â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘                                                                  â•‘
â•‘  Advanced Settings:                                              â•‘
â•‘    Temperature: [â–“â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘] 0.7                               â•‘
â•‘    Max Tokens: [4000         ]                                  â•‘
â•‘    [âœ“] Enable conversation history                              â•‘
â•‘    [âœ“] Cache responses                                          â•‘
â•‘                                                                  â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘                                                                  â•‘
â•‘  Local Model Options (Ollama):                                   â•‘
â•‘    Ollama URL: [http://localhost:11434]                         â•‘
â•‘    Available Models:                                             â•‘
â•‘      â€¢ codellama:13b (2.3 GB)                                   â•‘
â•‘      â€¢ deepseek-coder:6.7b (3.8 GB)                             â•‘
â•‘      â€¢ llama2:13b (7.4 GB)                                      â•‘
â•‘    [ðŸ“¥ Pull New Model]                                           â•‘
â•‘                                                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘         [Cancel]              [Save & Continue]                  â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Implementation:**

```python
# src/proxima/tui/screens/llm_settings.py

from textual.containers import Vertical, Horizontal, ScrollableContainer
from textual.widgets import Static, Button, Input, Select, Label, ProgressBar
from textual.screen import ModalScreen
import os
from pathlib import Path

from proxima.llm.providers import LLMProviderManager
from proxima.llm.models import get_available_models


class LLMSettingsScreen(ModalScreen):
    """LLM configuration screen."""
    
    DEFAULT_CSS = """
    LLMSettingsScreen {
        align: center middle;
    }
    
    LLMSettingsScreen .settings-container {
        width: 90;
        height: auto;
        border: double $primary;
        background: $surface;
        padding: 1 2;
    }
    
    LLMSettingsScreen .api-key-input {
        width: 100%;
    }
    
    LLMSettingsScreen .test-result {
        color: $success;
        margin: 1 0;
    }
    
    LLMSettingsScreen .error-result {
        color: $error;
        margin: 1 0;
    }
    """
    
    def __init__(self):
        super().__init__()
        self.provider_manager = LLMProviderManager()
        self.current_provider = None
        self.current_model = None
    
    def compose(self):
        """Compose the LLM settings screen."""
        with ScrollableContainer(classes="settings-container"):
            yield Static("LLM Configuration", classes="wizard-title")
            
            yield Static(
                "Configure AI model for backend generation:",
                classes="welcome-text"
            )
            
            yield Static(classes="section-divider")
            
            # Provider selection
            with Vertical(classes="form-field"):
                yield Label("Provider:", classes="field-label")
                yield Select(
                    [
                        ("OpenAI (API Key required)", "openai"),
                        ("Anthropic (API Key required)", "anthropic"),
                        ("Ollama (Local, no API key)", "ollama"),
                        ("LM Studio (Local, no API key)", "lmstudio"),
                        ("Groq (API Key required)", "groq"),
                    ],
                    value="openai",
                    id="select_provider",
                    classes="field-input"
                )
            
            # Model selection
            with Vertical(classes="form-field"):
                yield Label("Model:", classes="field-label")
                yield Select(
                    id="select_model",
                    classes="field-input"
                )
            
            # API Key input
            with Vertical(classes="form-field", id="api_key_section"):
                yield Label("API Key:", classes="field-label")
                with Horizontal():
                    yield Input(
                        placeholder="sk-...",
                        password=True,
                        id="input_api_key",
                        classes="api-key-input"
                    )
                    yield Button("ðŸ‘ Show", id="btn_toggle_key")
                yield Static(
                    "â„¹ Stored in: ~/.proxima/.env (encrypted)",
                    classes="field-hint"
                )
            
            # Test connection
            yield Button("Test Connection", id="btn_test", variant="primary")
            yield Static("", id="test_result")
            
            yield Static(classes="section-divider")
            
            # Advanced settings
            yield Static("Advanced Settings:", classes="field-label")
            with Vertical(classes="form-field"):
                yield Label("Temperature: 0.7")
                # Add slider widget here
                
                yield Label("Max Tokens:")
                yield Input(
                    placeholder="4000",
                    value="4000",
                    type="integer",
                    id="input_max_tokens"
                )
            
            # Navigation buttons
            with Horizontal(classes="button-container"):
                yield Button("Cancel", id="btn_cancel")
                yield Button(
                    "Save & Continue",
                    id="btn_save",
                    variant="primary"
                )
    
    async def on_select_changed(self, event: Select.Changed) -> None:
        """Handle provider/model selection."""
        if event.select.id == "select_provider":
            provider = event.value
            self.current_provider = provider
            
            # Update model list
            models = get_available_models(provider)
            model_select = self.query_one("#select_model", Select)
            model_select.set_options(
                [(m["name"], m["id"]) for m in models]
            )
            
            # Show/hide API key input
            api_key_section = self.query_one("#api_key_section")
            if provider in ["ollama", "lmstudio"]:
                api_key_section.display = False
            else:
                api_key_section.display = True
        
        elif event.select.id == "select_model":
            self.current_model = event.value
    
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses."""
        if event.button.id == "btn_test":
            await self._test_connection()
        
        elif event.button.id == "btn_toggle_key":
            api_key_input = self.query_one("#input_api_key", Input)
            api_key_input.password = not api_key_input.password
            event.button.label = "ðŸ‘ Hide" if not api_key_input.password else "ðŸ‘ Show"
        
        elif event.button.id == "btn_cancel":
            self.dismiss({"action": "cancel"})
        
        elif event.button.id == "btn_save":
            await self._save_configuration()
    
    async def _test_connection(self) -> None:
        """Test LLM connection."""
        test_result = self.query_one("#test_result", Static)
        test_result.update("Testing connection...")
        test_result.add_class("test-result")
        
        try:
            # Get API key
            api_key_input = self.query_one("#input_api_key", Input)
            api_key = api_key_input.value
            
            # Test connection
            success = await self.provider_manager.test_connection(
                provider=self.current_provider,
                model=self.current_model,
                api_key=api_key
            )
            
            if success:
                test_result.update("âœ“ Connection successful! Model is ready.")
                test_result.remove_class("error-result")
                test_result.add_class("test-result")
            else:
                test_result.update("âœ— Connection failed. Check your API key.")
                test_result.remove_class("test-result")
                test_result.add_class("error-result")
        
        except Exception as e:
            test_result.update(f"âœ— Error: {str(e)}")
            test_result.remove_class("test-result")
            test_result.add_class("error-result")
    
    async def _save_configuration(self) -> None:
        """Save LLM configuration."""
        api_key_input = self.query_one("#input_api_key", Input)
        max_tokens_input = self.query_one("#input_max_tokens", Input)
        
        config = {
            "provider": self.current_provider,
            "model": self.current_model,
            "api_key": api_key_input.value,
            "max_tokens": int(max_tokens_input.value or 4000),
            "temperature": 0.7,
        }
        
        # Save to config file
        await self.provider_manager.save_config(config)
        
        self.dismiss({"action": "saved", "config": config})
```

---

## Phase 1: Backend Addition Wizard

### Step 1: Welcome Screen

**File:** `src/proxima/tui/dialogs/backend_wizard/step_welcome.py`

**Purpose:** Introduce the wizard and let users select backend type

**UI Layout:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘               Add Custom Backend - Welcome                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘  Welcome to the Custom Backend Addition Wizard!                 â•‘
â•‘                                                                  â•‘
â•‘  This wizard will guide you through creating a new quantum      â•‘
â•‘  simulator backend for Proxima in 7 easy steps.                 â•‘
â•‘                                                                  â•‘
â•‘  No coding required - just answer a few questions!              â•‘
â•‘                                                                  â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘                                                                  â•‘
â•‘  Select your backend type:                                       â•‘
â•‘                                                                  â•‘
â•‘  â—‹ Python Library                                               â•‘
â•‘    Import and use an existing Python quantum simulator          â•‘
â•‘    Example: pyQuEST, ProjectQ, QuTiP                            â•‘
â•‘                                                                  â•‘
â•‘  â—‹ Command Line Tool                                            â•‘
â•‘    Execute external quantum simulator via command line          â•‘
â•‘    Example: QuEST binary, custom C++ simulator                  â•‘
â•‘                                                                  â•‘
â•‘  â—‹ API Server                                                   â•‘
â•‘    Connect to a remote quantum simulator API                    â•‘
â•‘    Example: IBM Quantum Cloud, AWS Braket                       â•‘
â•‘                                                                  â•‘
â•‘  â—‹ Custom Implementation                                        â•‘
â•‘    Fully custom backend with manual code entry                  â•‘
â•‘                                                                  â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘                                                                  â•‘
â•‘  Progress: Step 1 of 7                                          â•‘
â•‘  [â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 14%                                         â•‘
â•‘                                                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘         [Cancel]              [Next: Basic Info â†’]              â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Implementation Details:**

```python
# src/proxima/tui/dialogs/backend_wizard/step_welcome.py

from textual.containers import Vertical, Horizontal, Center
from textual.widgets import Static, Button, RadioButton, RadioSet
from textual.screen import ModalScreen
from rich.text import Text

from .wizard_state import BackendWizardState


class WelcomeStepScreen(ModalScreen):
    """Step 1: Welcome and backend type selection."""
    
    DEFAULT_CSS = """
    WelcomeStepScreen {
        align: center middle;
    }
    
    WelcomeStepScreen .wizard-container {
        width: 80;
        height: auto;
        border: double $primary;
        background: $surface;
        padding: 1 2;
    }
    
    WelcomeStepScreen .wizard-title {
        width: 100%;
        text-align: center;
        text-style: bold;
        color: $primary;
        margin-bottom: 1;
    }
    
    WelcomeStepScreen .welcome-text {
        width: 100%;
        margin: 1 0;
        color: $text;
    }
    
    WelcomeStepScreen .section-divider {
        width: 100%;
        height: 1;
        border-top: solid $primary-darken-2;
        margin: 1 0;
    }
    
    WelcomeStepScreen .backend-type-option {
        margin: 1 2;
    }
    
    WelcomeStepScreen .option-title {
        text-style: bold;
        color: $accent;
    }
    
    WelcomeStepScreen .option-description {
        color: $text-muted;
        margin-left: 4;
    }
    
    WelcomeStepScreen .progress-section {
        margin: 2 0 1 0;
    }
    
    WelcomeStepScreen .progress-text {
        color: $text-muted;
    }
    
    WelcomeStepScreen .button-container {
        width: 100%;
        height: auto;
        align: center middle;
        margin-top: 2;
    }
    
    WelcomeStepScreen .nav-button {
        margin: 0 1;
    }
    """
    
    def __init__(self, state: BackendWizardState):
        super().__init__()
        self.state = state
    
    def compose(self):
        """Compose the welcome screen."""
        with Center():
            with Vertical(classes="wizard-container"):
                # Title
                yield Static(
                    "Add Custom Backend - Welcome",
                    classes="wizard-title"
                )
                
                # Welcome message
                yield Static(
                    "Welcome to the Custom Backend Addition Wizard!\n\n"
                    "This wizard will guide you through creating a new quantum\n"
                    "simulator backend for Proxima in 7 easy steps.\n\n"
                    "No coding required - just answer a few questions!",
                    classes="welcome-text"
                )
                
                yield Static(classes="section-divider")
                
                # Backend type selection
                yield Static("Select your backend type:", classes="welcome-text")
                
                with RadioSet(id="backend_type_radio"):
                    with Vertical(classes="backend-type-option"):
                        yield RadioButton(
                            "Python Library",
                            value="python_library",
                            id="type_python"
                        )
                        yield Static(
                            "Import and use an existing Python quantum simulator\n"
                            "Example: pyQuEST, ProjectQ, QuTiP",
                            classes="option-description"
                        )
                    
                    with Vertical(classes="backend-type-option"):
                        yield RadioButton(
                            "Command Line Tool",
                            value="command_line",
                            id="type_cli"
                        )
                        yield Static(
                            "Execute external quantum simulator via command line\n"
                            "Example: QuEST binary, custom C++ simulator",
                            classes="option-description"
                        )
                    
                    with Vertical(classes="backend-type-option"):
                        yield RadioButton(
                            "API Server",
                            value="api_server",
                            id="type_api"
                        )
                        yield Static(
                            "Connect to a remote quantum simulator API\n"
                            "Example: IBM Quantum Cloud, AWS Braket",
                            classes="option-description"
                        )
                    
                    with Vertical(classes="backend-type-option"):
                        yield RadioButton(
                            "Custom Implementation",
                            value="custom",
                            id="type_custom"
                        )
                        yield Static(
                            "Fully custom backend with manual code entry",
                            classes="option-description"
                        )
                
                yield Static(classes="section-divider")
                
                # Progress indicator
                with Vertical(classes="progress-section"):
                    yield Static("Progress: Step 1 of 7", classes="progress-text")
                    yield Static("â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ 14%", classes="progress-text")
                
                # Navigation buttons
                with Horizontal(classes="button-container"):
                    yield Button("Cancel", id="btn_cancel", classes="nav-button")
                    yield Button(
                        "Next: Basic Info â†’",
                        id="btn_next",
                        variant="primary",
                        classes="nav-button"
                    )
    
    def on_mount(self):
        """Handle screen mount."""
        # Pre-select if state has a value
        if self.state.backend_type:
            radio_set = self.query_one("#backend_type_radio", RadioSet)
            radio_set.value = self.state.backend_type
    
    def on_radio_set_changed(self, event: RadioSet.Changed) -> None:
        """Handle backend type selection."""
        self.state.backend_type = event.pressed.value
        self.state.can_proceed = True
    
    def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses."""
        if event.button.id == "btn_cancel":
            self.dismiss({"action": "cancel"})
        elif event.button.id == "btn_next":
            if not self.state.backend_type:
                self.notify("Please select a backend type", severity="warning")
                return
            
            self.state.current_step = 2
            self.dismiss({"action": "next", "state": self.state})
```

### Step 2: Basic Information Input

**File:** `src/proxima/tui/dialogs/backend_wizard/step_basic_info.py`

**Purpose:** Collect basic metadata about the backend

**UI Layout:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘               Add Custom Backend - Basic Information             â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘  Please provide basic information about your backend:           â•‘
â•‘                                                                  â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘                                                                  â•‘
â•‘  Backend Name (internal identifier):                            â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘  â”‚ my_quantum_backend                                         â”‚ â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘  â„¹ Must be lowercase, no spaces (use underscores)              â•‘
â•‘                                                                  â•‘
â•‘  Display Name (shown in UI):                                    â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘  â”‚ My Quantum Backend                                         â”‚ â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘                                                                  â•‘
â•‘  Version:                                                        â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘  â”‚ 1.0.0                                                      â”‚ â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘                                                                  â•‘
â•‘  Description:                                                    â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘  â”‚ A custom quantum simulator backend for...                 â”‚ â•‘
â•‘  â”‚                                                            â”‚ â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘                                                                  â•‘
â•‘  Python Library/Module Name (if applicable):                    â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘  â”‚ my_quantum_lib                                             â”‚ â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘  â„¹ The Python package to import (e.g., 'qiskit', 'cirq')       â•‘
â•‘                                                                  â•‘
â•‘  Author/Maintainer (optional):                                  â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘  â”‚ Your Name                                                  â”‚ â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘                                                                  â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘                                                                  â•‘
â•‘  Progress: Step 2 of 7                                          â•‘
â•‘  [â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 29%                                           â•‘
â•‘                                                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘    [â† Back]          [Cancel]          [Next: Capabilities â†’]  â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Implementation Details:**

```python
# src/proxima/tui/dialogs/backend_wizard/step_basic_info.py

from textual.containers import Vertical, Horizontal, Center
from textual.widgets import Static, Button, Input, Label
from textual.screen import ModalScreen
from textual.validation import Function, ValidationResult, Validator
import re

from .wizard_state import BackendWizardState


class BackendNameValidator(Validator):
    """Validate backend name format."""
    
    def validate(self, value: str) -> ValidationResult:
        """Check if backend name is valid."""
        if not value:
            return self.failure("Backend name is required")
        
        if not re.match(r'^[a-z][a-z0-9_]*$', value):
            return self.failure(
                "Must start with lowercase letter, "
                "contain only lowercase letters, numbers, and underscores"
            )
        
        # Check if name already exists
        from proxima.backends.registry import BackendRegistry
        registry = BackendRegistry()
        if value in registry.list_backends():
            return self.failure(f"Backend '{value}' already exists")
        
        return self.success()


class VersionValidator(Validator):
    """Validate semantic version format."""
    
    def validate(self, value: str) -> ValidationResult:
        """Check if version follows semver."""
        if not value:
            return self.failure("Version is required")
        
        if not re.match(r'^\d+\.\d+\.\d+$', value):
            return self.failure("Must be in format: X.Y.Z (e.g., 1.0.0)")
        
        return self.success()


class BasicInfoStepScreen(ModalScreen):
    """Step 2: Basic information input."""
    
    DEFAULT_CSS = """
    BasicInfoStepScreen {
        align: center middle;
    }
    
    BasicInfoStepScreen .wizard-container {
        width: 90;
        height: auto;
        border: double $primary;
        background: $surface;
        padding: 1 2;
    }
    
    BasicInfoStepScreen .form-field {
        width: 100%;
        margin: 1 0;
    }
    
    BasicInfoStepScreen .field-label {
        color: $text;
        margin-bottom: 0;
    }
    
    BasicInfoStepScreen .field-input {
        width: 100%;
    }
    
    BasicInfoStepScreen .field-hint {
        color: $text-muted;
        margin-top: 0;
        margin-left: 2;
    }
    
    BasicInfoStepScreen .validation-error {
        color: $error;
        margin-left: 2;
    }
    """
    
    def __init__(self, state: BackendWizardState):
        super().__init__()
        self.state = state
    
    def compose(self):
        """Compose the basic info screen."""
        with Center():
            with Vertical(classes="wizard-container"):
                yield Static(
                    "Add Custom Backend - Basic Information",
                    classes="wizard-title"
                )
                
                yield Static(
                    "Please provide basic information about your backend:",
                    classes="welcome-text"
                )
                
                yield Static(classes="section-divider")
                
                # Backend Name
                with Vertical(classes="form-field"):
                    yield Label("Backend Name (internal identifier):", classes="field-label")
                    yield Input(
                        placeholder="my_quantum_backend",
                        value=self.state.backend_name,
                        validators=[BackendNameValidator()],
                        id="input_backend_name",
                        classes="field-input"
                    )
                    yield Static(
                        "â„¹ Must be lowercase, no spaces (use underscores)",
                        classes="field-hint"
                    )
                
                # Display Name
                with Vertical(classes="form-field"):
                    yield Label("Display Name (shown in UI):", classes="field-label")
                    yield Input(
                        placeholder="My Quantum Backend",
                        value=self.state.display_name,
                        id="input_display_name",
                        classes="field-input"
                    )
                
                # Version
                with Vertical(classes="form-field"):
                    yield Label("Version:", classes="field-label")
                    yield Input(
                        placeholder="1.0.0",
                        value=self.state.version,
                        validators=[VersionValidator()],
                        id="input_version",
                        classes="field-input"
                    )
                
                # Description
                with Vertical(classes="form-field"):
                    yield Label("Description:", classes="field-label")
                    yield Input(
                        placeholder="A custom quantum simulator backend for...",
                        value=self.state.description,
                        id="input_description",
                        classes="field-input"
                    )
                
                # Library Name
                with Vertical(classes="form-field"):
                    yield Label(
                        "Python Library/Module Name (if applicable):",
                        classes="field-label"
                    )
                    yield Input(
                        placeholder="my_quantum_lib",
                        value=self.state.library_name,
                        id="input_library_name",
                        classes="field-input"
                    )
                    yield Static(
                        "â„¹ The Python package to import (e.g., 'qiskit', 'cirq')",
                        classes="field-hint"
                    )
                
                # Author
                with Vertical(classes="form-field"):
                    yield Label("Author/Maintainer (optional):", classes="field-label")
                    yield Input(
                        placeholder="Your Name",
                        value=self.state.author,
                        id="input_author",
                        classes="field-input"
                    )
                
                yield Static(classes="section-divider")
                
                # Progress
                with Vertical(classes="progress-section"):
                    yield Static("Progress: Step 2 of 7", classes="progress-text")
                    yield Static("â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 29%", classes="progress-text")
                
                # Navigation
                with Horizontal(classes="button-container"):
                    yield Button("â† Back", id="btn_back", classes="nav-button")
                    yield Button("Cancel", id="btn_cancel", classes="nav-button")
                    yield Button(
                        "Next: Capabilities â†’",
                        id="btn_next",
                        variant="primary",
                        classes="nav-button"
                    )
    
    def on_input_changed(self, event: Input.Changed) -> None:
        """Handle input changes and update state."""
        input_id = event.input.id
        value = event.value
        
        if input_id == "input_backend_name":
            self.state.backend_name = value.lower().strip()
        elif input_id == "input_display_name":
            self.state.display_name = value.strip()
        elif input_id == "input_version":
            self.state.version = value.strip()
        elif input_id == "input_description":
            self.state.description = value.strip()
        elif input_id == "input_library_name":
            self.state.library_name = value.strip()
        elif input_id == "input_author":
            self.state.author = value.strip()
        
        # Auto-generate display name from backend name if empty
        if input_id == "input_backend_name" and not self.state.display_name:
            display_name_input = self.query_one("#input_display_name", Input)
            auto_display = value.replace('_', ' ').title()
            display_name_input.value = auto_display
            self.state.display_name = auto_display
        
        self._validate_form()
    
    def _validate_form(self) -> bool:
        """Validate all form fields."""
        backend_name_input = self.query_one("#input_backend_name", Input)
        version_input = self.query_one("#input_version", Input)
        
        # Check required fields
        if not self.state.backend_name:
            self.state.can_proceed = False
            return False
        
        if not self.state.display_name:
            self.state.can_proceed = False
            return False
        
        # Validate inputs
        if not backend_name_input.is_valid:
            self.state.can_proceed = False
            return False
        
        if not version_input.is_valid:
            self.state.can_proceed = False
            return False
        
        self.state.can_proceed = True
        return True
    
    def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses."""
        if event.button.id == "btn_back":
            self.state.current_step = 1
            self.dismiss({"action": "back", "state": self.state})
        
        elif event.button.id == "btn_cancel":
            self.dismiss({"action": "cancel"})
        
        elif event.button.id == "btn_next":
            if not self._validate_form():
                self.notify(
                    "Please fill in all required fields correctly",
                    severity="warning"
                )
                return
            
            self.state.current_step = 3
            self.dismiss({"action": "next", "state": self.state})
```

---

## Phase 1.5: AI-Powered Conversational Interface

### AI Chat Screen for Backend Creation

**File:** `src/proxima/tui/screens/backend_ai_chat.py`

**Purpose:** Conversational AI interface for creating backends through natural language

**UI Layout:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                AI-Powered Backend Creation                                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                              â•‘
â•‘  Chat with AI to create your custom backend                                 â•‘
â•‘  Model: GPT-4 | Provider: OpenAI | Status: ðŸŸ¢ Ready                         â•‘
â•‘                                                                              â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘                                                                              â•‘
â•‘  ðŸ’¬ Conversation:                                                            â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•‘
â•‘  â”‚                                                                      â”‚   â•‘
â•‘  â”‚ ðŸ¤– AI Assistant:                                                     â”‚   â•‘
â•‘  â”‚ Hello! I'll help you create a custom quantum backend for Proxima.   â”‚   â•‘
â•‘  â”‚ Can you describe the backend you'd like to add?                     â”‚   â•‘
â•‘  â”‚                                                                      â”‚   â•‘
â•‘  â”‚ ðŸ‘¤ You:                                                              â”‚   â•‘
â•‘  â”‚ I want to add a Python-based simulator called MyQuantum that        â”‚   â•‘
â•‘  â”‚ supports state vector simulations up to 25 qubits.                  â”‚   â•‘
â•‘  â”‚                                                                      â”‚   â•‘
â•‘  â”‚ ðŸ¤– AI Assistant:                                                     â”‚   â•‘
â•‘  â”‚ Great! I understand you want to create:                             â”‚   â•‘
â•‘  â”‚   â€¢ Name: MyQuantum                                                 â”‚   â•‘
â•‘  â”‚   â€¢ Type: Python library                                            â”‚   â•‘
â•‘  â”‚   â€¢ Simulator: State Vector                                         â”‚   â•‘
â•‘  â”‚   â€¢ Max Qubits: 25                                                  â”‚   â•‘
â•‘  â”‚                                                                      â”‚   â•‘
â•‘  â”‚ A few questions:                                                    â”‚   â•‘
â•‘  â”‚ 1. What's the Python import name? (e.g., 'myquantum')              â”‚   â•‘
â•‘  â”‚ 2. Does it support noise models?                                    â”‚   â•‘
â•‘  â”‚ 3. Any GPU acceleration?                                            â”‚   â•‘
â•‘  â”‚                                                                      â”‚   â•‘
â•‘  â”‚ ðŸ‘¤ You:                                                              â”‚   â•‘
â•‘  â”‚ 1. Import as 'myquantum_lib'                                        â”‚   â•‘
â•‘  â”‚ 2. Yes, supports noise                                              â”‚   â•‘
â•‘  â”‚ 3. No GPU                                                           â”‚   â•‘
â•‘  â”‚                                                                      â”‚   â•‘
â•‘  â”‚ ðŸ¤– AI Assistant:                                                     â”‚   â•‘
â•‘  â”‚ Perfect! I'm now generating the backend code...                     â”‚   â•‘
â•‘  â”‚ â³ Generating adapter.py...                                          â”‚   â•‘
â•‘  â”‚ âœ“ Generated adapter.py (245 lines)                                  â”‚   â•‘
â•‘  â”‚ â³ Generating normalizer.py...                                       â”‚   â•‘
â•‘  â”‚ âœ“ Generated normalizer.py (87 lines)                                â”‚   â•‘
â•‘  â”‚ â³ Generating __init__.py...                                         â”‚   â•‘
â•‘  â”‚ âœ“ Generated __init__.py (12 lines)                                  â”‚   â•‘
â•‘  â”‚ â³ Generating tests...                                               â”‚   â•‘
â•‘  â”‚ âœ“ Generated test_myquantum.py (156 lines)                           â”‚   â•‘
â•‘  â”‚                                                                      â”‚   â•‘
â•‘  â”‚ âœ“ Backend generated successfully!                                   â”‚   â•‘
â•‘  â”‚                                                                      â”‚   â•‘
â•‘  â”‚ Would you like to:                                                  â”‚   â•‘
â•‘  â”‚   [1] Review the generated code                                     â”‚   â•‘
â•‘  â”‚   [2] Test the backend                                              â”‚   â•‘
â•‘  â”‚   [3] Modify something                                              â”‚   â•‘
â•‘  â”‚   [4] Deploy the backend                                            â”‚   â•‘
â•‘  â”‚                                                                      â”‚   â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â•‘
â•‘                                                                              â•‘
â•‘  ðŸ’¬ Your message:                                                            â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•‘
â•‘  â”‚ Type your message here...                                            â”‚   â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â•‘
â•‘  [Send (Enter)] or [/help for commands]                                     â•‘
â•‘                                                                              â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘                                                                              â•‘
â•‘  Quick Actions:                                                              â•‘
â•‘  [ðŸ“‹ Review Code]  [ðŸ§ª Run Tests]  [ðŸ’¾ Deploy]  [âŒ Start Over]              â•‘
â•‘                                                                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  [â† Back to Mode Selection]  [Switch to Wizard Mode]  [âš™ LLM Settings]      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Implementation:**

```python
# src/proxima/tui/screens/backend_ai_chat.py

from textual.app import ComposeResult
from textual.containers import Vertical, Horizontal, ScrollableContainer
from textual.widgets import Static, Input, Button, RichLog
from textual.screen import Screen
from rich.text import Text
from rich.markdown import Markdown
from datetime import datetime

from proxima.llm.backend_agent import BackendCreationAgent
from proxima.llm.providers import LLMProviderManager
from .base import BaseScreen


class BackendAIChatScreen(BaseScreen):
    """AI-powered conversational backend creation interface."""
    
    SCREEN_NAME = "backend_ai_chat"
    SCREEN_TITLE = "AI Backend Creation"
    
    DEFAULT_CSS = """
    BackendAIChatScreen .chat-container {
        height: 100%;
        border: solid $primary;
        padding: 1;
    }
    
    BackendAIChatScreen .chat-log {
        height: 1fr;
        border: solid $primary-darken-2;
        padding: 1;
        background: $surface;
    }
    
    BackendAIChatScreen .message-user {
        color: $accent;
        margin: 1 0;
    }
    
    BackendAIChatScreen .message-ai {
        color: $primary;
        margin: 1 0;
    }
    
    BackendAIChatScreen .message-system {
        color: $warning;
        margin: 1 0;
        text-style: italic;
    }
    
    BackendAIChatScreen .message-success {
        color: $success;
        margin: 1 0;
    }
    
    BackendAIChatScreen .input-container {
        height: auto;
        padding: 1 0;
    }
    
    BackendAIChatScreen .message-input {
        width: 1fr;
    }
    
    BackendAIChatScreen .quick-actions {
        height: auto;
        layout: horizontal;
        padding: 1 0;
    }
    
    BackendAIChatScreen .quick-btn {
        margin-right: 1;
    }
    """
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.agent = None
        self.conversation_history = []
        self.backend_config = {}
        self.generated_code = {}
    
    def compose_main(self):
        """Compose the AI chat interface."""
        with Vertical(classes="chat-container"):
            # Header with model info
            yield Static(
                "Chat with AI to create your custom backend\n"
                "Model: GPT-4 | Provider: OpenAI | Status: ðŸŸ¢ Ready",
                classes="section-title"
            )
            
            # Chat log
            chat_log = RichLog(
                id="chat_log",
                classes="chat-log",
                highlight=True,
                markup=True
            )
            yield chat_log
            
            # Message input
            with Horizontal(classes="input-container"):
                yield Input(
                    placeholder="Type your message here...",
                    id="message_input",
                    classes="message-input"
                )
                yield Button(
                    "Send",
                    id="btn_send",
                    variant="primary"
                )
            
            yield Static(
                "[Send (Enter)] or [/help for commands]",
                classes="field-hint"
            )
            
            # Quick actions
            with Horizontal(classes="quick-actions"):
                yield Button("ðŸ“‹ Review Code", id="btn_review", classes="quick-btn")
                yield Button("ðŸ§ª Run Tests", id="btn_test", classes="quick-btn")
                yield Button("ðŸ’¾ Deploy", id="btn_deploy", classes="quick-btn")
                yield Button("âŒ Start Over", id="btn_reset", classes="quick-btn")
    
    async def on_mount(self):
        """Initialize the AI agent when screen mounts."""
        # Initialize LLM agent
        self.agent = BackendCreationAgent()
        await self.agent.initialize()
        
        # Send welcome message
        await self._add_ai_message(
            "Hello! I'll help you create a custom quantum backend for Proxima.\n\n"
            "Can you describe the backend you'd like to add?\n\n"
            "For example:\n"
            "â€¢ 'I want to add a Python simulator called MyQuantum'\n"
            "â€¢ 'Add support for QuEST library'\n"
            "â€¢ 'Create a backend for GPU-accelerated simulation'\n\n"
            "Or type '/help' for more options."
        )
    
    async def on_input_submitted(self, event: Input.Submitted):
        """Handle message submission."""
        if event.input.id == "message_input":
            await self._send_message(event.value)
            event.input.value = ""
    
    async def on_button_pressed(self, event: Button.Pressed):
        """Handle button presses."""
        button_id = event.button.id
        
        if button_id == "btn_send":
            message_input = self.query_one("#message_input", Input)
            await self._send_message(message_input.value)
            message_input.value = ""
        
        elif button_id == "btn_review":
            await self._review_code()
        
        elif button_id == "btn_test":
            await self._run_tests()
        
        elif button_id == "btn_deploy":
            await self._deploy_backend()
        
        elif button_id == "btn_reset":
            await self._reset_conversation()
    
    async def _send_message(self, message: str):
        """Send user message and get AI response."""
        if not message.strip():
            return
        
        # Add user message to chat
        await self._add_user_message(message)
        
        # Handle commands
        if message.startswith('/'):
            await self._handle_command(message)
            return
        
        # Show thinking indicator
        await self._add_system_message("ðŸ¤” AI is thinking...")
        
        try:
            # Get AI response
            response = await self.agent.process_message(
                message,
                conversation_history=self.conversation_history
            )
            
            # Remove thinking indicator
            chat_log = self.query_one("#chat_log", RichLog)
            chat_log.clear()
            
            # Re-add conversation history
            for msg in self.conversation_history:
                if msg['role'] == 'user':
                    chat_log.write(Text(f"ðŸ‘¤ You:\n{msg['content']}\n", style="bold cyan"))
                elif msg['role'] == 'assistant':
                    chat_log.write(Markdown(f"ðŸ¤– AI Assistant:\n{msg['content']}\n"))
            
            # Add new AI response
            await self._add_ai_message(response['message'])
            
            # Update backend config if provided
            if 'config_update' in response:
                self.backend_config.update(response['config_update'])
            
            # Update generated code if provided
            if 'generated_code' in response:
                self.generated_code.update(response['generated_code'])
                await self._add_success_message(
                    f"âœ“ Generated {len(response['generated_code'])} files"
                )
        
        except Exception as e:
            await self._add_system_message(f"âŒ Error: {str(e)}", error=True)
    
    async def _handle_command(self, command: str):
        """Handle special commands."""
        command = command.lower().strip()
        
        if command == '/help':
            await self._add_system_message(
                "Available commands:\n"
                "/help - Show this help\n"
                "/status - Show current backend configuration\n"
                "/reset - Start over\n"
                "/review - Review generated code\n"
                "/test - Run backend tests\n"
                "/deploy - Deploy the backend"
            )
        
        elif command == '/status':
            if self.backend_config:
                status_text = "Current Configuration:\n"
                for key, value in self.backend_config.items():
                    status_text += f"  â€¢ {key}: {value}\n"
                await self._add_system_message(status_text)
            else:
                await self._add_system_message("No configuration yet. Start describing your backend!")
        
        elif command == '/reset':
            await self._reset_conversation()
        
        elif command == '/review':
            await self._review_code()
        
        elif command == '/test':
            await self._run_tests()
        
        elif command == '/deploy':
            await self._deploy_backend()
    
    async def _add_user_message(self, message: str):
        """Add user message to chat."""
        chat_log = self.query_one("#chat_log", RichLog)
        chat_log.write(Text(f"ðŸ‘¤ You:\n{message}\n", style="bold cyan"))
        
        self.conversation_history.append({
            'role': 'user',
            'content': message,
            'timestamp': datetime.now().isoformat()
        })
    
    async def _add_ai_message(self, message: str):
        """Add AI message to chat."""
        chat_log = self.query_one("#chat_log", RichLog)
        chat_log.write(Markdown(f"ðŸ¤– AI Assistant:\n{message}\n"))
        
        self.conversation_history.append({
            'role': 'assistant',
            'content': message,
            'timestamp': datetime.now().isoformat()
        })
    
    async def _add_system_message(self, message: str, error: bool = False):
        """Add system message to chat."""
        chat_log = self.query_one("#chat_log", RichLog)
        style = "bold red" if error else "bold yellow"
        chat_log.write(Text(f"âš™ System:\n{message}\n", style=style))
    
    async def _add_success_message(self, message: str):
        """Add success message to chat."""
        chat_log = self.query_one("#chat_log", RichLog)
        chat_log.write(Text(f"âœ“ {message}\n", style="bold green"))
    
    async def _review_code(self):
        """Review generated code."""
        if not self.generated_code:
            await self._add_system_message("No code generated yet!")
            return
        
        # Show code preview dialog
        from ..dialogs.code_preview import CodePreviewDialog
        
        await self.app.push_screen(
            CodePreviewDialog(self.generated_code),
            callback=lambda result: None
        )
    
    async def _run_tests(self):
        """Run backend tests."""
        if not self.generated_code:
            await self._add_system_message("No code generated yet!")
            return
        
        await self._add_system_message("ðŸ§ª Running tests...")
        
        # Run tests using agent
        test_results = await self.agent.run_tests(self.generated_code)
        
        if test_results['success']:
            await self._add_success_message(
                f"âœ“ All tests passed! ({test_results['passed']}/{test_results['total']})"
            )
        else:
            await self._add_system_message(
                f"âŒ Tests failed: {test_results['failed']}/{test_results['total']}\n"
                f"Errors: {test_results['errors']}",
                error=True
            )
    
    async def _deploy_backend(self):
        """Deploy the backend."""
        if not self.generated_code:
            await self._add_system_message("No code generated yet!")
            return
        
        await self._add_system_message("ðŸ’¾ Deploying backend...")
        
        # Deploy using agent
        deploy_result = await self.agent.deploy_backend(
            self.backend_config,
            self.generated_code
        )
        
        if deploy_result['success']:
            await self._add_success_message(
                f"âœ“ Backend deployed successfully!\n"
                f"Location: {deploy_result['path']}\n"
                f"Files created: {len(self.generated_code)}"
            )
            
            # Navigate back to backends screen
            self.notify("Backend added successfully!", severity="success")
            self.app.pop_screen()
        else:
            await self._add_system_message(
                f"âŒ Deployment failed: {deploy_result['error']}",
                error=True
            )
    
    async def _reset_conversation(self):
        """Reset the conversation."""
        self.conversation_history = []
        self.backend_config = {}
        self.generated_code = {}
        
        chat_log = self.query_one("#chat_log", RichLog)
        chat_log.clear()
        
        await self._add_ai_message(
            "Conversation reset. Let's start fresh!\n\n"
            "What kind of backend would you like to create?"
        )
```

### LLM Backend Agent

**File:** `src/proxima/llm/backend_agent.py`

**Purpose:** Specialized AI agent for backend creation

```python
# src/proxima/llm/backend_agent.py

from typing import Dict, List, Any, Optional
import json
from pathlib import Path

from .providers import LLMProviderManager
from .prompts import get_backend_generation_prompt
from ..tui.controllers.backend_generator import BackendCodeGenerator
from ..tui.dialogs.backend_wizard.wizard_state import BackendWizardState


class BackendCreationAgent:
    """AI agent specialized in creating quantum backends."""
    
    def __init__(self, config: Optional[Dict] = None):
        """Initialize the backend creation agent."""
        self.config = config or {}
        self.provider_manager = LLMProviderManager()
        self.conversation_state = {
            'phase': 'initial',  # initial, gathering_info, generating_code, testing, deploying
            'backend_config': {},
            'missing_info': [],
            'confidence': 0.0
        }
    
    async def initialize(self):
        """Initialize the LLM provider."""
        await self.provider_manager.initialize()
    
    async def process_message(
        self,
        user_message: str,
        conversation_history: List[Dict]
    ) -> Dict[str, Any]:
        """
        Process user message and generate response.
        
        Returns:
            Dict with 'message', 'config_update', 'generated_code', 'next_action'
        """
        # Build context from conversation history
        context = self._build_context(conversation_history)
        
        # Create prompt
        prompt = get_backend_generation_prompt(
            user_message=user_message,
            context=context,
            current_config=self.conversation_state['backend_config'],
            phase=self.conversation_state['phase']
        )
        
        # Get LLM response
        llm_response = await self.provider_manager.generate(
            messages=[
                {"role": "system", "content": prompt['system']},
                *conversation_history,
                {"role": "user", "content": user_message}
            ],
            temperature=0.7,
            max_tokens=4000
        )
        
        # Parse response
        parsed_response = self._parse_llm_response(llm_response)
        
        # Update conversation state
        self._update_state(parsed_response)
        
        # Generate code if ready
        if parsed_response.get('ready_to_generate', False):
            generated_code = await self._generate_backend_code()
            parsed_response['generated_code'] = generated_code
        
        return parsed_response
    
    def _build_context(self, conversation_history: List[Dict]) -> str:
        """Build context from conversation history."""
        context_parts = []
        
        # Add existing backend info if any
        if self.conversation_state['backend_config']:
            context_parts.append("Current Configuration:")
            context_parts.append(json.dumps(self.conversation_state['backend_config'], indent=2))
        
        # Add missing info
        if self.conversation_state['missing_info']:
            context_parts.append("\nStill need to know:")
            for item in self.conversation_state['missing_info']:
                context_parts.append(f"  - {item}")
        
        return "\n".join(context_parts)
    
    def _parse_llm_response(self, llm_response: str) -> Dict[str, Any]:
        """Parse LLM response and extract structured data."""
        result = {
            'message': llm_response,
            'config_update': {},
            'ready_to_generate': False,
            'next_action': None
        }
        
        # Try to extract JSON config from response
        try:
            # Look for JSON blocks in markdown
            if '```json' in llm_response:
                json_start = llm_response.find('```json') + 7
                json_end = llm_response.find('```', json_start)
                json_str = llm_response[json_start:json_end].strip()
                config = json.loads(json_str)
                result['config_update'] = config
        except:
            pass
        
        # Check if ready to generate
        if 'generate' in llm_response.lower() or 'create' in llm_response.lower():
            # Verify we have minimum required info
            required_fields = ['backend_name', 'backend_type', 'simulator_types']
            has_required = all(
                field in self.conversation_state['backend_config']
                for field in required_fields
            )
            result['ready_to_generate'] = has_required
        
        return result
    
    def _update_state(self, parsed_response: Dict):
        """Update conversation state based on parsed response."""
        if parsed_response['config_update']:
            self.conversation_state['backend_config'].update(
                parsed_response['config_update']
            )
        
        # Update phase
        if parsed_response['ready_to_generate']:
            self.conversation_state['phase'] = 'generating_code'
        elif self.conversation_state['backend_config']:
            self.conversation_state['phase'] = 'gathering_info'
    
    async def _generate_backend_code(self) -> Dict[str, str]:
        """Generate backend code from current configuration."""
        # Create wizard state from config
        state = BackendWizardState()
        config = self.conversation_state['backend_config']
        
        state.backend_name = config.get('backend_name', '')
        state.display_name = config.get('display_name', config.get('backend_name', ''))
        state.backend_type = config.get('backend_type', 'python_library')
        state.library_name = config.get('library_name', '')
        state.version = config.get('version', '1.0.0')
        state.description = config.get('description', '')
        state.simulator_types = config.get('simulator_types', ['state_vector'])
        state.max_qubits = config.get('max_qubits', 20)
        state.supports_noise = config.get('supports_noise', False)
        state.supports_gpu = config.get('supports_gpu', False)
        
        # Generate code
        generator = BackendCodeGenerator(state)
        success, file_paths, file_contents = generator.generate_all_files()
        
        if success:
            return file_contents
        else:
            raise Exception("Code generation failed")
    
    async def run_tests(self, generated_code: Dict[str, str]) -> Dict[str, Any]:
        """Run tests on generated backend code."""
        # Implementation would run actual tests
        # For now, return mock results
        return {
            'success': True,
            'total': 5,
            'passed': 5,
            'failed': 0,
            'errors': []
        }
    
    async def deploy_backend(
        self,
        config: Dict,
        generated_code: Dict[str, str]
    ) -> Dict[str, Any]:
        """Deploy the backend to the file system."""
        try:
            # Create generator
            state = BackendWizardState()
            state.backend_name = config['backend_name']
            # ... set other fields
            
            generator = BackendCodeGenerator(state)
            success = generator.write_files_to_disk(generated_code)
            
            if success:
                return {
                    'success': True,
                    'path': str(generator.output_dir)
                }
            else:
                return {
                    'success': False,
                    'error': 'Failed to write files'
                }
        
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
```

---

### Step 3: Capabilities Configuration

**File:** `src/proxima/tui/dialogs/backend_wizard/step_capabilities.py`

**Purpose:** Configure backend capabilities and features

**UI Layout:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘               Add Custom Backend - Capabilities                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘  Configure what your backend can do:                             â•‘
â•‘                                                                  â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘                                                                  â•‘
â•‘  Simulator Types (select all that apply):                        â•‘
â•‘    [âœ“] State Vector Simulation                                  â•‘
â•‘    [âœ“] Density Matrix Simulation                                â•‘
â•‘    [ ] Tensor Network Simulation                                â•‘
â•‘    [ ] Custom Simulation Type                                   â•‘
â•‘                                                                  â•‘
â•‘  Maximum Qubits Supported:                                       â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘  â”‚ 20                                                         â”‚ â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘  â„¹ Typical range: 10-30 for CPU, 30-50 for GPU                 â•‘
â•‘                                                                  â•‘
â•‘  Additional Features:                                            â•‘
â•‘    [âœ“] Noise Model Support                                      â•‘
â•‘    [ ] GPU Acceleration                                         â•‘
â•‘    [ ] Batch Execution                                          â•‘
â•‘    [ ] Parameter Binding                                        â•‘
â•‘    [ ] Custom Gate Definitions                                  â•‘
â•‘                                                                  â•‘
â•‘  Performance Characteristics:                                    â•‘
â•‘    Estimated memory per qubit: [  Auto-calculate  ]             â•‘
â•‘    Expected execution speed:   [ â–¼ Medium        ]              â•‘
â•‘                                                                  â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘                                                                  â•‘
â•‘  Progress: Step 3 of 7                                          â•‘
â•‘  [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 43%                                             â•‘
â•‘                                                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘    [â† Back]          [Cancel]          [Next: Gate Mapping â†’]  â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Implementation Details:**

```python
# src/proxima/tui/dialogs/backend_wizard/step_capabilities.py

from textual.containers import Vertical, Horizontal, Center, Grid
from textual.widgets import Static, Button, Input, Checkbox, Select, Label
from textual.screen import ModalScreen

from .wizard_state import BackendWizardState


class CapabilitiesStepScreen(ModalScreen):
    """Step 3: Capabilities configuration."""
    
    DEFAULT_CSS = """
    CapabilitiesStepScreen {
        align: center middle;
    }
    
    CapabilitiesStepScreen .checkbox-group {
        margin: 1 2;
    }
    
    CapabilitiesStepScreen .capability-checkbox {
        margin: 0 0 0 2;
    }
    """
    
    def __init__(self, state: BackendWizardState):
        super().__init__()
        self.state = state
        self.checkboxes = {}
    
    def compose(self):
        """Compose the capabilities screen."""
        with Center():
            with Vertical(classes="wizard-container"):
                yield Static(
                    "Add Custom Backend - Capabilities",
                    classes="wizard-title"
                )
                
                yield Static(
                    "Configure what your backend can do:",
                    classes="welcome-text"
                )
                
                yield Static(classes="section-divider")
                
                # Simulator Types
                yield Static("Simulator Types (select all that apply):", classes="field-label")
                with Vertical(classes="checkbox-group"):
                    yield Checkbox(
                        "State Vector Simulation",
                        value="state_vector" in self.state.simulator_types,
                        id="cb_state_vector"
                    )
                    yield Checkbox(
                        "Density Matrix Simulation",
                        value="density_matrix" in self.state.simulator_types,
                        id="cb_density_matrix"
                    )
                    yield Checkbox(
                        "Tensor Network Simulation",
                        value="tensor_network" in self.state.simulator_types,
                        id="cb_tensor_network"
                    )
                    yield Checkbox(
                        "Custom Simulation Type",
                        value="custom" in self.state.simulator_types,
                        id="cb_custom_sim"
                    )
                
                # Max Qubits
                with Vertical(classes="form-field"):
                    yield Label("Maximum Qubits Supported:", classes="field-label")
                    yield Input(
                        placeholder="20",
                        value=str(self.state.max_qubits),
                        type="integer",
                        id="input_max_qubits",
                        classes="field-input"
                    )
                    yield Static(
                        "â„¹ Typical range: 10-30 for CPU, 30-50 for GPU",
                        classes="field-hint"
                    )
                
                # Additional Features
                yield Static("Additional Features:", classes="field-label")
                with Vertical(classes="checkbox-group"):
                    yield Checkbox(
                        "Noise Model Support",
                        value=self.state.supports_noise,
                        id="cb_noise"
                    )
                    yield Checkbox(
                        "GPU Acceleration",
                        value=self.state.supports_gpu,
                        id="cb_gpu"
                    )
                    yield Checkbox(
                        "Batch Execution",
                        value=self.state.supports_batching,
                        id="cb_batching"
                    )
                
                yield Static(classes="section-divider")
                
                # Progress
                with Vertical(classes="progress-section"):
                    yield Static("Progress: Step 3 of 7", classes="progress-text")
                    yield Static("â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ 43%", classes="progress-text")
                
                # Navigation
                with Horizontal(classes="button-container"):
                    yield Button("â† Back", id="btn_back", classes="nav-button")
                    yield Button("Cancel", id="btn_cancel", classes="nav-button")
                    yield Button(
                        "Next: Gate Mapping â†’",
                        id="btn_next",
                        variant="primary",
                        classes="nav-button"
                    )
    
    def on_checkbox_changed(self, event: Checkbox.Changed) -> None:
        """Handle checkbox changes."""
        checkbox_id = event.checkbox.id
        is_checked = event.value
        
        # Update simulator types
        sim_type_map = {
            "cb_state_vector": "state_vector",
            "cb_density_matrix": "density_matrix",
            "cb_tensor_network": "tensor_network",
            "cb_custom_sim": "custom"
        }
        
        if checkbox_id in sim_type_map:
            sim_type = sim_type_map[checkbox_id]
            if is_checked and sim_type not in self.state.simulator_types:
                self.state.simulator_types.append(sim_type)
            elif not is_checked and sim_type in self.state.simulator_types:
                self.state.simulator_types.remove(sim_type)
        
        # Update features
        elif checkbox_id == "cb_noise":
            self.state.supports_noise = is_checked
        elif checkbox_id == "cb_gpu":
            self.state.supports_gpu = is_checked
        elif checkbox_id == "cb_batching":
            self.state.supports_batching = is_checked
        
        self._validate_capabilities()
    
    def on_input_changed(self, event: Input.Changed) -> None:
        """Handle input changes."""
        if event.input.id == "input_max_qubits":
            try:
                self.state.max_qubits = int(event.value) if event.value else 20
            except ValueError:
                self.state.max_qubits = 20
    
    def _validate_capabilities(self) -> bool:
        """Validate capabilities configuration."""
        # At least one simulator type must be selected
        if not self.state.simulator_types:
            self.state.can_proceed = False
            return False
        
        self.state.can_proceed = True
        return True
    
    def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses."""
        if event.button.id == "btn_back":
            self.state.current_step = 2
            self.dismiss({"action": "back", "state": self.state})
        
        elif event.button.id == "btn_cancel":
            self.dismiss({"action": "cancel"})
        
        elif event.button.id == "btn_next":
            if not self._validate_capabilities():
                self.notify(
                    "Please select at least one simulator type",
                    severity="warning"
                )
                return
            
            self.state.current_step = 4
            self.dismiss({"action": "next", "state": self.state})
```

### Step 4: Gate Mapping Configuration

**File:** `src/proxima/tui/dialogs/backend_wizard/step_gate_mapping.py`

**Purpose:** Configure how Proxima gates map to backend gates

**UI Layout:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘               Add Custom Backend - Gate Mapping                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘  Configure how Proxima gates map to your backend:                â•‘
â•‘                                                                  â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘                                                                  â•‘
â•‘  Gate Mapping Mode:                                              â•‘
â•‘    â—‹ Automatic (recommended)                                     â•‘
â•‘      Use standard gate names (H, X, Y, Z, CNOT, etc.)           â•‘
â•‘                                                                  â•‘
â•‘    â—‹ Use Template                                                â•‘
â•‘      Select from common backend templates (Qiskit, Cirq, etc.)  â•‘
â•‘                                                                  â•‘
â•‘    â—‹ Manual Mapping                                              â•‘
â•‘      Define custom gate mappings                                 â•‘
â•‘                                                                  â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘                                                                  â•‘
â•‘  [Automatic mode selected]                                       â•‘
â•‘                                                                  â•‘
â•‘  Standard gates will be automatically mapped:                    â•‘
â•‘    â€¢ Single-qubit: H, X, Y, Z, S, T, Rx, Ry, Rz                 â•‘
â•‘    â€¢ Two-qubit: CNOT, CZ, SWAP                                  â•‘
â•‘    â€¢ Three-qubit: TOFFOLI, FREDKIN                              â•‘
â•‘                                                                  â•‘
â•‘  â„¹ You can customize individual gate mappings in the code       â•‘
â•‘    template step if needed.                                      â•‘
â•‘                                                                  â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘                                                                  â•‘
â•‘  Progress: Step 4 of 7                                          â•‘
â•‘  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘] 57%                                              â•‘
â•‘                                                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘    [â† Back]          [Cancel]          [Next: Code Template â†’] â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

*[Continue with Step 5-7 implementations...]*

---

## Phase 2: Backend Configuration Interface

### Wizard State Manager

**File:** `src/proxima/tui/dialogs/backend_wizard/wizard_state.py`

```python
# src/proxima/tui/dialogs/backend_wizard/wizard_state.py

from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional


@dataclass
class BackendWizardState:
    """State container for the backend addition wizard."""
    
    # Step 1: Backend Type
    backend_type: str = ""  # 'python_library', 'command_line', 'api_server', 'custom'
    
    # Step 2: Basic Information
    backend_name: str = ""
    display_name: str = ""
    version: str = "1.0.0"
    description: str = ""
    author: str = ""
    library_name: str = ""
    
    # Step 3: Capabilities
    simulator_types: List[str] = field(default_factory=list)
    max_qubits: int = 20
    supports_noise: bool = False
    supports_gpu: bool = False
    supports_batching: bool = False
    custom_features: Dict[str, Any] = field(default_factory=dict)
    
    # Step 4: Gate Mapping
    gate_mapping_mode: str = "auto"  # 'auto', 'manual', 'template'
    supported_gates: List[str] = field(default_factory=list)
    custom_gate_mappings: Dict[str, str] = field(default_factory=dict)
    gate_template: str = ""  # 'qiskit', 'cirq', 'custom'
    
    # Step 5: Code Template
    template_type: str = "basic"  # 'basic', 'advanced', 'custom'
    custom_initialization_code: str = ""
    custom_execution_code: str = ""
    custom_conversion_code: str = ""
    
    # Step 6: Testing
    test_circuit: str = "bell_state"
    test_shots: int = 1024
    test_results: Optional[Dict] = None
    validation_passed: bool = False
    validation_errors: List[str] = field(default_factory=list)
    
    # Step 7: Review
    files_to_create: List[str] = field(default_factory=list)
    file_previews: Dict[str, str] = field(default_factory=dict)
    generation_successful: bool = False
    installation_path: str = ""
    
    # Navigation
    current_step: int = 1
    total_steps: int = 7
    can_proceed: bool = False
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    
    def reset(self):
        """Reset wizard to initial state."""
        self.__init__()
    
    def get_progress_percentage(self) -> int:
        """Get current progress as percentage."""
        return int((self.current_step / self.total_steps) * 100)
    
    def get_progress_bar(self, width: int = 10) -> str:
        """Get visual progress bar."""
        filled = int((self.current_step / self.total_steps) * width)
        return "â–ˆ" * filled + "â–‘" * (width - filled)
    
    def validate_current_step(self) -> bool:
        """Validate current step data."""
        validators = {
            1: self._validate_step1,
            2: self._validate_step2,
            3: self._validate_step3,
            4: self._validate_step4,
            5: self._validate_step5,
            6: self._validate_step6,
            7: self._validate_step7,
        }
        
        validator = validators.get(self.current_step)
        if validator:
            return validator()
        return False
    
    def _validate_step1(self) -> bool:
        """Validate step 1: Backend type selection."""
        return self.backend_type in ["python_library", "command_line", "api_server", "custom"]
    
    def _validate_step2(self) -> bool:
        """Validate step 2: Basic information."""
        return (
            bool(self.backend_name) and
            bool(self.display_name) and
            bool(self.version)
        )
    
    def _validate_step3(self) -> bool:
        """Validate step 3: Capabilities."""
        return (
            len(self.simulator_types) > 0 and
            self.max_qubits > 0
        )
    
    def _validate_step4(self) -> bool:
        """Validate step 4: Gate mapping."""
        return self.gate_mapping_mode in ["auto", "manual", "template"]
    
    def _validate_step5(self) -> bool:
        """Validate step 5: Code template."""
        return self.template_type in ["basic", "advanced", "custom"]
    
    def _validate_step6(self) -> bool:
        """Validate step 6: Testing."""
        return self.validation_passed
    
    def _validate_step7(self) -> bool:
        """Validate step 7: Review."""
        return self.generation_successful
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert state to dictionary."""
        return {
            "backend_type": self.backend_type,
            "backend_name": self.backend_name,
            "display_name": self.display_name,
            "version": self.version,
            "description": self.description,
            "author": self.author,
            "library_name": self.library_name,
            "simulator_types": self.simulator_types,
            "max_qubits": self.max_qubits,
            "supports_noise": self.supports_noise,
            "supports_gpu": self.supports_gpu,
            "supports_batching": self.supports_batching,
            "custom_features": self.custom_features,
            "gate_mapping_mode": self.gate_mapping_mode,
            "supported_gates": self.supported_gates,
            "custom_gate_mappings": self.custom_gate_mappings,
            "template_type": self.template_type,
        }
```

---

## Phase 3: Code Generation System

### Backend Code Generator

**File:** `src/proxima/tui/controllers/backend_generator.py`

```python
# src/proxima/tui/controllers/backend_generator.py

from pathlib import Path
from typing import Dict, List, Tuple
from ..dialogs.backend_wizard.wizard_state import BackendWizardState
from ..utils.backend_templates import BackendTemplateEngine


class BackendCodeGenerator:
    """Generate backend code files from wizard configuration."""
    
    def __init__(self, state: BackendWizardState):
        """Initialize generator with wizard state."""
        self.state = state
        self.template_engine = BackendTemplateEngine()
        self.output_dir = Path("src/proxima/backends") / state.backend_name
    
    def generate_all_files(self) -> Tuple[bool, List[str], Dict[str, str]]:
        """
        Generate all backend files.
        
        Returns:
            Tuple of (success, file_paths, file_contents)
        """
        try:
            files = {}
            
            # Generate adapter.py
            adapter_code = self._generate_adapter()
            files[f"{self.state.backend_name}/adapter.py"] = adapter_code
            
            # Generate normalizer.py
            normalizer_code = self._generate_normalizer()
            files[f"{self.state.backend_name}/normalizer.py"] = normalizer_code
            
            # Generate __init__.py
            init_code = self._generate_init()
            files[f"{self.state.backend_name}/__init__.py"] = init_code
            
            # Generate README.md
            readme_code = self._generate_readme()
            files[f"{self.state.backend_name}/README.md"] = readme_code
            
            # Generate tests
            test_code = self._generate_tests()
            files[f"tests/backends/test_{self.state.backend_name}.py"] = test_code
            
            file_paths = list(files.keys())
            
            return True, file_paths, files
            
        except Exception as e:
            return False, [], {"error": str(e)}
    
    def _generate_adapter(self) -> str:
        """Generate adapter.py content."""
        template = self.template_engine.get_adapter_template(self.state.backend_type)
        
        return template.render(
            backend_name=self.state.backend_name,
            display_name=self.state.display_name,
            version=self.state.version,
            description=self.state.description,
            library_name=self.state.library_name,
            simulator_types=self.state.simulator_types,
            max_qubits=self.state.max_qubits,
            supports_noise=self.state.supports_noise,
            supports_gpu=self.state.supports_gpu,
            supports_batching=self.state.supports_batching,
            custom_init=self.state.custom_initialization_code,
            custom_execute=self.state.custom_execution_code,
        )
    
    def _generate_normalizer(self) -> str:
        """Generate normalizer.py content."""
        template = self.template_engine.get_normalizer_template()
        
        return template.render(
            backend_name=self.state.backend_name,
            display_name=self.state.display_name,
        )
    
    def _generate_init(self) -> str:
        """Generate __init__.py content."""
        return self.template_engine.get_init_template().render(
            backend_name=self.state.backend_name,
            adapter_class=f"{self.state.backend_name.title().replace('_', '')}Adapter",
            normalizer_class=f"{self.state.backend_name.title().replace('_', '')}Normalizer",
        )
    
    def _generate_readme(self) -> str:
        """Generate README.md content."""
        return self.template_engine.get_readme_template().render(
            display_name=self.state.display_name,
            description=self.state.description,
            author=self.state.author,
            version=self.state.version,
            library_name=self.state.library_name,
        )
    
    def _generate_tests(self) -> str:
        """Generate test file content."""
        return self.template_engine.get_test_template().render(
            backend_name=self.state.backend_name,
            display_name=self.state.display_name,
        )
    
    def write_files_to_disk(self, files: Dict[str, str]) -> bool:
        """
        Write generated files to disk.
        
        Args:
            files: Dictionary of file_path -> content
        
        Returns:
            True if successful
        """
        try:
            base_path = Path("src/proxima/backends")
            base_path.mkdir(parents=True, exist_ok=True)
            
            for file_path, content in files.items():
                full_path = base_path.parent.parent / file_path
                full_path.parent.mkdir(parents=True, exist_ok=True)
                
                with open(full_path, 'w', encoding='utf-8') as f:
                    f.write(content)
            
            return True
            
        except Exception as e:
            print(f"Error writing files: {e}")
            return False
```

### Template Engine

**File:** `src/proxima/tui/utils/backend_templates.py`

```python
# src/proxima/tui/utils/backend_templates.py

from jinja2 import Template


class BackendTemplateEngine:
    """Template engine for generating backend code."""
    
    def get_adapter_template(self, backend_type: str) -> Template:
        """Get adapter template based on backend type."""
        
        if backend_type == "python_library":
            return Template(PYTHON_LIBRARY_ADAPTER_TEMPLATE)
        elif backend_type == "command_line":
            return Template(COMMAND_LINE_ADAPTER_TEMPLATE)
        elif backend_type == "api_server":
            return Template(API_SERVER_ADAPTER_TEMPLATE)
        else:
            return Template(CUSTOM_ADAPTER_TEMPLATE)
    
    def get_normalizer_template(self) -> Template:
        """Get normalizer template."""
        return Template(NORMALIZER_TEMPLATE)
    
    def get_init_template(self) -> Template:
        """Get __init__.py template."""
        return Template(INIT_TEMPLATE)
    
    def get_readme_template(self) -> Template:
        """Get README.md template."""
        return Template(README_TEMPLATE)
    
    def get_test_template(self) -> Template:
        """Get test file template."""
        return Template(TEST_TEMPLATE)


# ============================================================================
# TEMPLATE DEFINITIONS
# ============================================================================

PYTHON_LIBRARY_ADAPTER_TEMPLATE = '''"""{{ display_name }} Backend Adapter.

Auto-generated by Proxima Backend Wizard.
Backend Type: Python Library
Version: {{ version }}
{% if author %}Author: {{ author }}{% endif %}
"""

from typing import Any, Dict, List
from proxima.backends.base import (
    BaseBackendAdapter,
    Capabilities,
    SimulatorType,
    ValidationResult,
    ResourceEstimate,
    ExecutionResult,
    ResultType,
)


class {{ backend_name.title().replace('_', '') }}Adapter(BaseBackendAdapter):
    """Adapter for {{ display_name }}."""
    
    name = "{{ backend_name }}"
    version = "{{ version }}"
    
    def __init__(self, config: Dict[str, Any] = None):
        """Initialize the {{ display_name }} adapter."""
        self.config = config or {}
        self._simulator = None
        self._initialized = False
    
    def get_name(self) -> str:
        """Return backend identifier."""
        return self.name
    
    def get_version(self) -> str:
        """Return backend version string."""
        return self.version
    
    def get_capabilities(self) -> Capabilities:
        """Return supported capabilities."""
        return Capabilities(
            simulator_types=[
                {% for sim_type in simulator_types %}
                SimulatorType.{{ sim_type.upper() }},
                {% endfor %}
            ],
            max_qubits={{ max_qubits }},
            supports_noise={{ supports_noise|lower }},
            supports_gpu={{ supports_gpu|lower }},
            supports_batching={{ supports_batching|lower }},
        )
    
    def initialize(self) -> None:
        """Initialize the backend."""
        if self._initialized:
            return
        
        {% if library_name %}
        try:
            import {{ library_name }}
            self._simulator = {{ library_name }}.Simulator()
            self._initialized = True
        except ImportError as e:
            raise RuntimeError(
                f"{{ library_name }} not installed. "
                f"Install with: pip install {{ library_name }}"
            ) from e
        {% else %}
        # Custom initialization code
        {{ custom_init if custom_init else "pass" }}
        self._initialized = True
        {% endif %}
    
    def validate_circuit(self, circuit: Any) -> ValidationResult:
        """Validate circuit compatibility with the backend."""
        # Basic validation
        if not circuit:
            return ValidationResult(
                valid=False,
                message="Circuit is None or empty"
            )
        
        # Check qubit count
        if hasattr(circuit, 'qubit_count'):
            if circuit.qubit_count > {{ max_qubits }}:
                return ValidationResult(
                    valid=False,
                    message=f"Circuit has {circuit.qubit_count} qubits, "
                           f"maximum is {{ max_qubits }}"
                )
        
        return ValidationResult(valid=True)
    
    def estimate_resources(self, circuit: Any) -> ResourceEstimate:
        """Estimate resources for execution."""
        qubit_count = getattr(circuit, 'qubit_count', 0)
        
        # Memory estimate: 2^n * 16 bytes for state vector
        memory_mb = (2 ** qubit_count * 16) / (1024 * 1024)
        
        # Time estimate: rough approximation
        gate_count = getattr(circuit, 'gate_count', 0)
        time_ms = gate_count * 0.1  # 0.1ms per gate
        
        return ResourceEstimate(
            memory_mb=memory_mb,
            time_ms=time_ms,
        )
    
    def execute(
        self,
        circuit: Any,
        options: Dict[str, Any] = None
    ) -> ExecutionResult:
        """Execute a circuit and return results."""
        if not self._initialized:
            self.initialize()
        
        options = options or {}
        shots = options.get('shots', 1024)
        
        import time
        start_time = time.time()
        
        {% if custom_execute %}
        # Custom execution code
        {{ custom_execute }}
        {% else %}
        # Default execution
        # Convert circuit to backend format
        native_circuit = self._convert_circuit(circuit)
        
        # Execute
        raw_result = self._simulator.run(native_circuit, shots=shots)
        {% endif %}
        
        execution_time = (time.time() - start_time) * 1000  # Convert to ms
        
        # Create result
        return ExecutionResult(
            backend=self.name,
            simulator_type=SimulatorType.STATE_VECTOR,
            execution_time_ms=execution_time,
            qubit_count=getattr(circuit, 'qubit_count', 0),
            shot_count=shots,
            result_type=ResultType.COUNTS,
            data={"counts": raw_result.get('counts', {})},
            raw_result=raw_result,
        )
    
    def supports_simulator(self, sim_type: SimulatorType) -> bool:
        """Return whether the simulator type is supported."""
        supported = [
            {% for sim_type in simulator_types %}
            SimulatorType.{{ sim_type.upper() }},
            {% endfor %}
        ]
        return sim_type in supported
    
    def is_available(self) -> bool:
        """Return whether the backend is available on this system."""
        {% if library_name %}
        try:
            import {{ library_name }}
            return True
        except ImportError:
            return False
        {% else %}
        return True
        {% endif %}
    
    def _convert_circuit(self, circuit: Any) -> Any:
        """Convert Proxima circuit to backend format."""
        # TODO: Implement circuit conversion
        return circuit
    
    def cleanup(self) -> None:
        """Clean up backend resources."""
        if self._simulator:
            if hasattr(self._simulator, 'close'):
                self._simulator.close()
            self._simulator = None
        self._initialized = False
'''


NORMALIZER_TEMPLATE = '''"""Result normalizer for {{ display_name }}.

Auto-generated by Proxima Backend Wizard.
"""

from typing import Dict, Any
from proxima.core.result import ExecutionResult


class {{ backend_name.title().replace('_', '') }}Normalizer:
    """Normalize results from {{ display_name }}."""
    
    def normalize(self, raw_result: Any) -> ExecutionResult:
        """
        Convert backend-specific result to Proxima format.
        
        Args:
            raw_result: Raw result from {{ display_name }}
        
        Returns:
            Normalized ExecutionResult
        """
        # Extract counts from raw result
        counts = {}
        
        if isinstance(raw_result, dict):
            counts = raw_result.get('counts', {})
        elif hasattr(raw_result, 'measurements'):
            counts = raw_result.measurements
        
        # Normalize state strings to binary format
        normalized_counts = {}
        for state, count in counts.items():
            normalized_state = self._normalize_state(state)
            normalized_counts[normalized_state] = count
        
        return {
            'counts': normalized_counts,
            'shots': sum(normalized_counts.values()),
        }
    
    def _normalize_state(self, state: str) -> str:
        """Normalize state string representation."""
        # Remove any prefix/suffix
        state = str(state).strip("|<> ")
        
        # Ensure binary format
        if state.isdigit() and set(state).issubset({'0', '1'}):
            return state
        
        # Convert from int if needed
        try:
            return format(int(state, 2), 'b')
        except ValueError:
            return state
'''


INIT_TEMPLATE = '''"""{{ backend_name.title().replace('_', ' ') }} backend module.

Auto-generated by Proxima Backend Wizard.
"""

from .adapter import {{ adapter_class }}
from .normalizer import {{ normalizer_class }}

__all__ = ["{{ adapter_class }}", "{{ normalizer_class }}"]
'''


README_TEMPLATE = '''# {{ display_name }}

{{ description }}

## Installation

```bash
{% if library_name %}
pip install {{ library_name }}
{% else %}
# Follow installation instructions for your backend
{% endif %}
```

## Usage

```python
from proxima.backends.{{ backend_name }} import {{ backend_name.title().replace('_', '') }}Adapter

# Initialize adapter
adapter = {{ backend_name.title().replace('_', '') }}Adapter()
adapter.initialize()

# Execute circuit
result = adapter.execute(circuit, options={'shots': 1024})
```

## Configuration

Configuration options for {{ display_name }}:

- `shots`: Number of measurement shots (default: 1024)
- Add more configuration options here...

## Metadata

- **Version**: {{ version }}
{% if author %}- **Author**: {{ author }}{% endif %}
- **Auto-generated**: Yes
- **Generator**: Proxima Backend Wizard

## License

Same as Proxima project.
'''


TEST_TEMPLATE = '''"""Tests for {{ display_name }} backend.

Auto-generated by Proxima Backend Wizard.
"""

import pytest
from proxima.backends.{{ backend_name }} import {{ backend_name.title().replace('_', '') }}Adapter


@pytest.fixture
def adapter():
    """Create {{ display_name }} adapter instance."""
    adapter = {{ backend_name.title().replace('_', '') }}Adapter()
    adapter.initialize()
    yield adapter
    adapter.cleanup()


def test_adapter_initialization(adapter):
    """Test adapter initializes correctly."""
    assert adapter.get_name() == "{{ backend_name }}"
    assert adapter.is_available()


def test_get_capabilities(adapter):
    """Test capabilities reporting."""
    caps = adapter.get_capabilities()
    assert caps.max_qubits > 0
    assert len(caps.simulator_types) > 0


def test_validate_circuit(adapter):
    """Test circuit validation."""
    # Create a simple test circuit
    # TODO: Implement with actual circuit
    pass


def test_execute_circuit(adapter):
    """Test circuit execution."""
    # Create a simple test circuit
    # TODO: Implement with actual circuit
    pass


def test_cleanup(adapter):
    """Test cleanup."""
    adapter.cleanup()
    # Verify cleanup worked
'''


COMMAND_LINE_ADAPTER_TEMPLATE = '''# Command line backend adapter template
# Similar structure to Python library template but uses subprocess
'''


API_SERVER_ADAPTER_TEMPLATE = '''# API server backend adapter template  
# Uses requests/httpx for API calls
'''


CUSTOM_ADAPTER_TEMPLATE = '''# Custom backend adapter template
# Minimal template for full customization
'''
```

---

## Phase 4: Testing & Validation Interface

### Step 6: Testing Screen

**File:** `src/proxima/tui/dialogs/backend_wizard/step_testing.py`

**Purpose:** Test the generated backend code before deployment

**UI Layout:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘               Add Custom Backend - Testing                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘  Test your backend before deployment:                            â•‘
â•‘                                                                  â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘                                                                  â•‘
â•‘  Select test circuit:                                            â•‘
â•‘    [â–¼ Bell State Circuit        ]                               â•‘
â•‘                                                                  â•‘
â•‘  Number of shots:                                                â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘  â”‚ 1024                                                       â”‚ â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘                                                                  â•‘
â•‘  [        Run Test        ]                                     â•‘
â•‘                                                                  â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘                                                                  â•‘
â•‘  Test Results:                                                   â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘  â”‚ âœ“ Backend initialization: SUCCESS                         â”‚ â•‘
â•‘  â”‚ âœ“ Circuit validation: SUCCESS                             â”‚ â•‘
â•‘  â”‚ âœ“ Circuit execution: SUCCESS                              â”‚ â•‘
â•‘  â”‚ âœ“ Result normalization: SUCCESS                           â”‚ â•‘
â•‘  â”‚                                                            â”‚ â•‘
â•‘  â”‚ Execution time: 24ms                                       â”‚ â•‘
â•‘  â”‚ Results:                                                   â”‚ â•‘
â•‘  â”‚   |00âŸ©: 512 (50.0%)                                        â”‚ â•‘
â•‘  â”‚   |11âŸ©: 512 (50.0%)                                        â”‚ â•‘
â•‘  â”‚                                                            â”‚ â•‘
â•‘  â”‚ âœ“ All tests passed!                                        â”‚ â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘                                                                  â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘                                                                  â•‘
â•‘  Progress: Step 6 of 7                                          â•‘
â•‘  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 86%                                               â•‘
â•‘                                                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘    [â† Back]          [Cancel]          [Next: Review â†’]        â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Phase 5: Integration & Deployment

### Step 7: Review & Deploy

**File:** `src/proxima/tui/dialogs/backend_wizard/step_review.py`

**Purpose:** Final review and deployment of the backend

**UI Layout:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘               Add Custom Backend - Review & Deploy               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘  Review and deploy your custom backend:                          â•‘
â•‘                                                                  â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘                                                                  â•‘
â•‘  Backend Summary:                                                â•‘
â•‘    Name: My Quantum Backend                                     â•‘
â•‘    Internal ID: my_quantum_backend                              â•‘
â•‘    Version: 1.0.0                                               â•‘
â•‘    Type: Python Library                                         â•‘
â•‘    Simulator Types: State Vector, Density Matrix               â•‘
â•‘    Max Qubits: 20                                               â•‘
â•‘                                                                  â•‘
â•‘  Files to be created:                                            â•‘
â•‘    âœ“ src/proxima/backends/my_quantum_backend/adapter.py        â•‘
â•‘    âœ“ src/proxima/backends/my_quantum_backend/normalizer.py     â•‘
â•‘    âœ“ src/proxima/backends/my_quantum_backend/__init__.py       â•‘
â•‘    âœ“ src/proxima/backends/my_quantum_backend/README.md         â•‘
â•‘    âœ“ tests/backends/test_my_quantum_backend.py                 â•‘
â•‘                                                                  â•‘
â•‘  Registry Integration:                                           â•‘
â•‘    âœ“ Backend will be auto-registered on next Proxima start     â•‘
â•‘    âœ“ Available in backend selection menus                      â•‘
â•‘                                                                  â•‘
â•‘  [ View Generated Code ]                                        â•‘
â•‘                                                                  â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘
â•‘                                                                  â•‘
â•‘  Progress: Step 7 of 7                                          â•‘
â•‘  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%                                              â•‘
â•‘                                                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘    [â† Back]          [Cancel]          [ðŸš€ Deploy Backend]     â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Complete File Structure

```
src/proxima/
â”œâ”€â”€ tui/
â”‚   â”œâ”€â”€ screens/
â”‚   â”‚   â”œâ”€â”€ backends.py                     # MODIFY: Add "Add Backend" button
â”‚   â”‚   â””â”€â”€ backend_wizard.py               # NEW: Main wizard coordinator
â”‚   â”‚
â”‚   â”œâ”€â”€ dialogs/
â”‚   â”‚   â””â”€â”€ backend_wizard/                 # NEW: Wizard dialog components
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ wizard_state.py             # NEW: State management
â”‚   â”‚       â”œâ”€â”€ step_welcome.py             # NEW: Step 1
â”‚   â”‚       â”œâ”€â”€ step_basic_info.py          # NEW: Step 2
â”‚   â”‚       â”œâ”€â”€ step_capabilities.py        # NEW: Step 3
â”‚   â”‚       â”œâ”€â”€ step_gate_mapping.py        # NEW: Step 4
â”‚   â”‚       â”œâ”€â”€ step_code_template.py       # NEW: Step 5
â”‚   â”‚       â”œâ”€â”€ step_testing.py             # NEW: Step 6
â”‚   â”‚       â””â”€â”€ step_review.py              # NEW: Step 7
â”‚   â”‚
â”‚   â”œâ”€â”€ controllers/
â”‚   â”‚   â””â”€â”€ backend_generator.py            # NEW: Code generation
â”‚   â”‚
â”‚   â”œâ”€â”€ widgets/
â”‚   â”‚   â”œâ”€â”€ wizard_navigation.py            # NEW: Navigation controls
â”‚   â”‚   â””â”€â”€ code_preview.py                 # NEW: Code preview widget
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ backend_templates.py            # NEW: Jinja2 templates
â”‚
â””â”€â”€ backends/
    â”œâ”€â”€ registry.py                         # MODIFY: Auto-discovery of new backends
    â””â”€â”€ _generated/                         # NEW: Generated backends directory
        â””â”€â”€ .gitignore

tests/
â””â”€â”€ tui/
    â””â”€â”€ test_backend_wizard.py              # NEW: Wizard tests
```

---

## Implementation Checklist

### Phase 1: Foundation (Week 1)
- [ ] Create wizard state management (`wizard_state.py`)
- [ ] Create navigation widget (`wizard_navigation.py`)
- [ ] Create Step 1: Welcome screen (`step_welcome.py`)
- [ ] Test wizard navigation flow

### Phase 2: Data Collection (Week 2)
- [ ] Create Step 2: Basic info screen (`step_basic_info.py`)
- [ ] Implement form validation
- [ ] Create Step 3: Capabilities screen (`step_capabilities.py`)
- [ ] Create Step 4: Gate mapping screen (`step_gate_mapping.py`)
- [ ] Test all input validation

### Phase 3: Code Generation (Week 3)
- [ ] Create template engine (`backend_templates.py`)
- [ ] Create code generator (`backend_generator.py`)
- [ ] Implement Python library template
- [ ] Implement command line template
- [ ] Implement API server template
- [ ] Create Step 5: Code template screen (`step_code_template.py`)
- [ ] Test code generation

### Phase 4: Testing & Deployment (Week 4)
- [ ] Create Step 6: Testing screen (`step_testing.py`)
- [ ] Implement backend test runner
- [ ] Create Step 7: Review screen (`step_review.py`)
- [ ] Create code preview widget (`code_preview.py`)
- [ ] Implement file writing system
- [ ] Test full wizard flow

### Phase 5: Integration (Week 5)
- [ ] Update backends screen with "Add Backend" button
- [ ] Update registry for auto-discovery
- [ ] Create wizard coordinator (`backend_wizard.py`)
- [ ] Add comprehensive error handling
- [ ] Write documentation
- [ ] Create user guide

### Phase 6: Polish (Week 6)
- [ ] Add keyboard shortcuts
- [ ] Improve UI styling
- [ ] Add tooltips and help text
- [ ] Implement undo/redo functionality
- [ ] Add export/import configuration
- [ ] Final testing and bug fixes

---

## Testing Procedures

### Unit Tests

```python
# tests/tui/test_backend_wizard.py

import pytest
from proxima.tui.dialogs.backend_wizard.wizard_state import BackendWizardState
from proxima.tui.controllers.backend_generator import BackendCodeGenerator


def test_wizard_state_initialization():
    """Test wizard state initializes correctly."""
    state = BackendWizardState()
    assert state.current_step == 1
    assert state.total_steps == 7
    assert not state.can_proceed


def test_wizard_state_validation():
    """Test step validation."""
    state = BackendWizardState()
    
    # Step 1 should fail without backend type
    assert not state.validate_current_step()
    
    # Step 1 should pass with backend type
    state.backend_type = "python_library"
    assert state.validate_current_step()


def test_backend_code_generation():
    """Test backend code generation."""
    state = BackendWizardState()
    state.backend_name = "test_backend"
    state.display_name = "Test Backend"
    state.version = "1.0.0"
    state.backend_type = "python_library"
    state.simulator_types = ["state_vector"]
    state.max_qubits = 20
    
    generator = BackendCodeGenerator(state)
    success, files, contents = generator.generate_all_files()
    
    assert success
    assert len(files) >= 4
    assert "test_backend/adapter.py" in files
    assert "test_backend/normalizer.py" in files


@pytest.mark.asyncio
async def test_wizard_navigation():
    """Test wizard navigation flow."""
    # This would test the actual TUI navigation
    # Requires textual testing framework
    pass
```

### Integration Tests

```python
def test_full_wizard_flow():
    """Test complete wizard flow from start to finish."""
    state = BackendWizardState()
    
    # Step 1
    state.backend_type = "python_library"
    state.current_step = 2
    
    # Step 2
    state.backend_name = "my_backend"
    state.display_name = "My Backend"
    state.version = "1.0.0"
    state.library_name = "my_lib"
    state.current_step = 3
    
    # Step 3
    state.simulator_types = ["state_vector"]
    state.max_qubits = 20
    state.current_step = 4
    
    # ... continue through all steps
    
    # Verify final state
    assert state.current_step == 7
    assert state.validate_current_step()
```

---

## Success Criteria

âœ… **User Experience**
- User can add a new backend in under 5 minutes (wizard mode)
- User can add a new backend in under 3 minutes (AI chat mode)
- No Python coding required in either mode
- Clear error messages and validation
- Beautiful, intuitive UI with smooth transitions
- AI provides helpful suggestions and explanations

âœ… **AI Features**
- Natural language backend description works correctly
- AI asks relevant clarifying questions
- Generated code is syntactically correct and follows conventions
- AI can handle ambiguous requests and ask for clarification
- Conversation context is maintained throughout session
- AI suggestions improve based on user feedback

âœ… **LLM Integration**
- Support for multiple LLM providers (OpenAI, Anthropic, local models)
- Secure API key storage and management
- Graceful fallback if LLM is unavailable
- Token usage tracking and cost estimation
- Response streaming for better UX
- Error handling for API failures

âœ… **Functionality**
- Generates working backend code (both modes)
- Integrates with existing Proxima architecture
- Passes all validation tests
- Auto-registers with backend registry
- AI-generated code passes all tests
- Code review interface shows readable diffs

âœ… **Code Quality**
- Generated code follows Proxima conventions
- Proper error handling
- Comprehensive docstrings
- Type hints throughout
- AI-generated code is well-structured and commented
- Code complexity is reasonable

âœ… **Testing**
- All wizard steps have unit tests
- AI chat interface has integration tests
- LLM mock tests for offline testing
- Generated backends pass standard test suite
- AI response parsing is robust
- Error recovery mechanisms work correctly

âœ… **Configuration Management**
- LLM settings persist between sessions
- Environment variables properly handled
- Config files encrypted for security
- Easy provider/model switching
- Local model support works without API keys
- Conversation history can be exported

---

## Additional Implementation: LLM Prompts

### Prompt Template System

**File:** `src/proxima/llm/prompts/__init__.py`

```python
# src/proxima/llm/prompts/__init__.py

from pathlib import Path


def get_backend_generation_prompt(
    user_message: str,
    context: str,
    current_config: dict,
    phase: str
) -> dict:
    """Generate prompt for backend creation based on conversation phase."""
    
    system_prompt = f"""You are an expert quantum computing backend developer assistant for Proxima.

Your role is to help users create custom quantum simulator backends through conversation.

Current Phase: {phase}
Current Configuration: {current_config}
Context: {context}

Guidelines:
1. Be conversational and helpful
2. Ask clarifying questions if information is missing
3. Suggest reasonable defaults
4. Explain technical decisions clearly
5. Generate clean, working Python code
6. Follow Proxima's architecture patterns

When you have enough information, generate backend code following this structure:
- adapter.py: Main backend adapter class
- normalizer.py: Result normalization
- __init__.py: Package initialization
- tests: Comprehensive test suite

Required Information:
- Backend name (internal identifier)
- Display name (user-facing)
- Backend type (python_library, command_line, api_server, custom)
- Library/module name (if applicable)
- Simulator types (state_vector, density_matrix, etc.)
- Maximum qubits supported
- Special features (noise, GPU, batching)

Respond in a friendly, technical tone. Use markdown formatting for code blocks.
When providing configuration, use JSON format in ```json code blocks.
"""
    
    return {
        'system': system_prompt,
        'user': user_message
    }


def get_code_refinement_prompt(
    original_code: str,
    user_feedback: str,
    file_path: str
) -> dict:
    """Generate prompt for refining generated code."""
    
    system_prompt = f"""You are refining code for a Proxima quantum backend.

File: {file_path}
User Feedback: {user_feedback}

Original Code:
```python
{original_code}
```

Make the requested changes while:
1. Maintaining code structure and style
2. Keeping existing functionality intact
3. Following Python best practices
4. Adding helpful comments
5. Ensuring type hints are present

Respond with only the updated code in a ```python code block.
"""
    
    return {
        'system': system_prompt,
        'user': f"Modify the code based on this feedback: {user_feedback}"
    }


def get_capability_suggestion_prompt(
    backend_description: str,
    backend_type: str
) -> dict:
    """Generate prompt for suggesting backend capabilities."""
    
    system_prompt = f"""You are helping determine capabilities for a quantum backend.

Backend Description: {backend_description}
Backend Type: {backend_type}

Based on the description, suggest:
1. Maximum qubits (realistic estimate)
2. Simulator types supported
3. Additional features (noise, GPU, batching, etc.)
4. Performance characteristics

Respond with JSON:
```json
{{
  "max_qubits": <number>,
  "simulator_types": [<list of types>],
  "supports_noise": <boolean>,
  "supports_gpu": <boolean>,
  "supports_batching": <boolean>,
  "reasoning": "<explanation>"
}}
```
"""
    
    return {
        'system': system_prompt,
        'user': f"Suggest capabilities for: {backend_description}"
    }


def get_error_debugging_prompt(
    error_message: str,
    code_snippet: str,
    context: str
) -> dict:
    """Generate prompt for debugging errors."""
    
    system_prompt = f"""You are helping debug a Proxima backend error.

Error: {error_message}

Code:
```python
{code_snippet}
```

Context: {context}

Analyze the error and provide:
1. Root cause explanation
2. Suggested fix
3. Updated code snippet
4. Prevention tips

Be concise but thorough.
"""
    
    return {
        'system': system_prompt,
        'user': f"Help me fix this error: {error_message}"
    }
```

### LLM Provider Manager

**File:** `src/proxima/llm/providers.py`

```python
# src/proxima/llm/providers.py

from typing import Dict, List, Optional, Any
import os
import httpx
from pathlib import Path
import yaml
from cryptography.fernet import Fernet


class LLMProviderManager:
    """Manage LLM providers and API connections."""
    
    def __init__(self, config_path: Optional[Path] = None):
        """Initialize provider manager."""
        self.config_path = config_path or Path.home() / ".proxima" / "llm_config.yaml"
        self.config = {}
        self.current_provider = None
        self.current_model = None
        self.cipher_suite = None
    
    async def initialize(self):
        """Initialize the provider manager."""
        # Load config
        if self.config_path.exists():
            with open(self.config_path) as f:
                self.config = yaml.safe_load(f)
        
        # Set up encryption
        key_file = self.config_path.parent / ".key"
        if key_file.exists():
            with open(key_file, 'rb') as f:
                key = f.read()
        else:
            key = Fernet.generate_key()
            key_file.parent.mkdir(parents=True, exist_ok=True)
            with open(key_file, 'wb') as f:
                f.write(key)
        
        self.cipher_suite = Fernet(key)
        
        # Set current provider
        llm_config = self.config.get('llm', {})
        self.current_provider = llm_config.get('default_provider', 'openai')
        self.current_model = llm_config.get('default_model', 'gpt-4')
    
    async def generate(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 4000,
        stream: bool = False
    ) -> str:
        """Generate completion from LLM."""
        provider_config = self.config['llm']['providers'][self.current_provider]
        
        if provider_config['type'] == 'openai':
            return await self._generate_openai(messages, temperature, max_tokens, stream)
        elif provider_config['type'] == 'anthropic':
            return await self._generate_anthropic(messages, temperature, max_tokens, stream)
        elif provider_config['type'] == 'ollama':
            return await self._generate_ollama(messages, temperature, max_tokens, stream)
        else:
            raise ValueError(f"Unsupported provider type: {provider_config['type']}")
    
    async def _generate_openai(
        self,
        messages: List[Dict],
        temperature: float,
        max_tokens: int,
        stream: bool
    ) -> str:
        """Generate using OpenAI API."""
        provider_config = self.config['llm']['providers'][self.current_provider]
        api_key = self._get_api_key(provider_config)
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{provider_config['base_url']}/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.current_model,
                    "messages": messages,
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                    "stream": stream
                },
                timeout=60.0
            )
            
            if response.status_code != 200:
                raise Exception(f"API error: {response.text}")
            
            result = response.json()
            return result['choices'][0]['message']['content']
    
    async def _generate_anthropic(
        self,
        messages: List[Dict],
        temperature: float,
        max_tokens: int,
        stream: bool
    ) -> str:
        """Generate using Anthropic API."""
        provider_config = self.config['llm']['providers'][self.current_provider]
        api_key = self._get_api_key(provider_config)
        
        # Convert messages to Anthropic format
        system_message = ""
        anthropic_messages = []
        
        for msg in messages:
            if msg['role'] == 'system':
                system_message = msg['content']
            else:
                anthropic_messages.append(msg)
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{provider_config['base_url']}/messages",
                headers={
                    "x-api-key": api_key,
                    "anthropic-version": "2023-06-01",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.current_model,
                    "system": system_message,
                    "messages": anthropic_messages,
                    "temperature": temperature,
                    "max_tokens": max_tokens
                },
                timeout=60.0
            )
            
            if response.status_code != 200:
                raise Exception(f"API error: {response.text}")
            
            result = response.json()
            return result['content'][0]['text']
    
    async def _generate_ollama(
        self,
        messages: List[Dict],
        temperature: float,
        max_tokens: int,
        stream: bool
    ) -> str:
        """Generate using Ollama (local)."""
        provider_config = self.config['llm']['providers'][self.current_provider]
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{provider_config['base_url']}/api/chat",
                json={
                    "model": self.current_model,
                    "messages": messages,
                    "stream": stream,
                    "options": {
                        "temperature": temperature,
                        "num_predict": max_tokens
                    }
                },
                timeout=120.0
            )
            
            if response.status_code != 200:
                raise Exception(f"API error: {response.text}")
            
            result = response.json()
            return result['message']['content']
    
    def _get_api_key(self, provider_config: Dict) -> str:
        """Get API key from environment or encrypted storage."""
        # Try environment variable first
        api_key_env = provider_config.get('api_key_env')
        if api_key_env and os.getenv(api_key_env):
            return os.getenv(api_key_env)
        
        # Try encrypted storage
        key_file = self.config_path.parent / f".{self.current_provider}_key"
        if key_file.exists():
            with open(key_file, 'rb') as f:
                encrypted_key = f.read()
            return self.cipher_suite.decrypt(encrypted_key).decode()
        
        raise ValueError(f"No API key found for {self.current_provider}")
    
    async def test_connection(
        self,
        provider: str,
        model: str,
        api_key: Optional[str] = None
    ) -> bool:
        """Test connection to LLM provider."""
        try:
            # Temporarily save API key if provided
            if api_key:
                old_provider = self.current_provider
                old_model = self.current_model
                
                self.current_provider = provider
                self.current_model = model
                
                # Save encrypted key
                key_file = self.config_path.parent / f".{provider}_key"
                key_file.parent.mkdir(parents=True, exist_ok=True)
                encrypted_key = self.cipher_suite.encrypt(api_key.encode())
                with open(key_file, 'wb') as f:
                    f.write(encrypted_key)
            
            # Test with simple message
            response = await self.generate(
                messages=[
                    {"role": "user", "content": "Hello, are you working?"}
                ],
                max_tokens=10
            )
            
            return bool(response)
        
        except Exception:
            return False
        
        finally:
            if api_key:
                self.current_provider = old_provider
                self.current_model = old_model
    
    async def save_config(self, config: Dict):
        """Save LLM configuration."""
        # Update config
        if 'llm' not in self.config:
            self.config['llm'] = {}
        
        self.config['llm']['default_provider'] = config['provider']
        self.config['llm']['default_model'] = config['model']
        
        # Save to file
        self.config_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.config_path, 'w') as f:
            yaml.safe_dump(self.config, f)
        
        # Save encrypted API key
        if config.get('api_key'):
            key_file = self.config_path.parent / f".{config['provider']}_key"
            encrypted_key = self.cipher_suite.encrypt(config['api_key'].encode())
            with open(key_file, 'wb') as f:
                f.write(encrypted_key)
```

---

## End of Document

This document provides complete specifications for AI-powered backend creation in Proxima TUI.

### Key Features Implemented:

1. **Dual Mode Interface**: Traditional wizard OR AI chat
2. **Multi-LLM Support**: OpenAI, Anthropic, Ollama, LM Studio
3. **Conversational Backend Creation**: Natural language descriptions
4. **Smart Code Generation**: AI generates complete backend code
5. **Interactive Testing**: Real-time validation and testing
6. **Secure Configuration**: Encrypted API key storage
7. **Context-Aware**: AI remembers conversation history
8. **Beautiful TUI**: Professional terminal interface

### Implementation Phases:

- **Phase 0**: Mode selection and LLM configuration âœ“
- **Phase 1**: Traditional wizard (7 steps) âœ“  
- **Phase 1.5**: AI chat interface âœ“
- **Phase 2**: Backend configuration âœ“
- **Phase 3**: Code generation (AI-powered) âœ“
- **Phase 4**: Testing & validation âœ“
- **Phase 5**: Deployment âœ“

**Implementation Ready**: YES âœ“  
**AI Agent Compatible**: YES âœ“  
**LLM Integration**: COMPLETE âœ“  
**Production Ready**: After implementation and testing âœ“

---

**Total Document Length**: ~4000 lines  
**Code Examples**: 15+ complete implementations  
**UI Mockups**: 10+ detailed layouts  
**Ready for**: Immediate AI agent implementation
